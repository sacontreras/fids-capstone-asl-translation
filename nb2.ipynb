{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Anaconda: learn-env",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "import apache_beam.transforms.sql\n",
    "\n",
    "import beam__common\n",
    "import fidscs_globals\n",
    "import random\n",
    "\n",
    "from importlib import import_module\n",
    "data_extractor = import_module('data-extractor', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "use_beam: True\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CollectiveCommunication.AUTO\n",
      "Number of devices available for parallel processing: 1\n",
      "PipelineOptions:\n",
      "{'runner': 'DirectRunner', 'streaming': False, 'beam_services': {}, 'type_check_strictness': 'DEFAULT_TO_ANY', 'type_check_additional': '', 'pipeline_type_check': True, 'runtime_type_check': False, 'performance_runtime_type_check': False, 'direct_runner_use_stacked_bundle': True, 'direct_runner_bundle_repeat': 0, 'direct_num_workers': 0, 'direct_running_mode': 'multi_threading', 'dataflow_endpoint': 'https://dataflow.googleapis.com', 'project': 'my-project', 'job_name': None, 'staging_location': None, 'temp_location': None, 'region': None, 'service_account_email': None, 'no_auth': False, 'template_location': None, 'labels': None, 'update': False, 'transform_name_mapping': None, 'enable_streaming_engine': False, 'dataflow_kms_key': None, 'flexrs_goal': None, 'hdfs_host': None, 'hdfs_port': None, 'hdfs_user': None, 'hdfs_full_urls': False, 'num_workers': None, 'max_num_workers': None, 'autoscaling_algorithm': None, 'machine_type': None, 'disk_size_gb': None, 'disk_type': None, 'worker_region': None, 'worker_zone': None, 'zone': None, 'network': None, 'subnetwork': None, 'worker_harness_container_image': None, 'sdk_harness_container_image_overrides': None, 'use_public_ips': None, 'min_cpu_platform': None, 'dataflow_worker_jar': None, 'dataflow_job_file': None, 'experiments': None, 'number_of_worker_harness_threads': None, 'profile_cpu': False, 'profile_memory': False, 'profile_location': None, 'profile_sample_rate': 1.0, 'requirements_file': None, 'requirements_cache': None, 'setup_file': None, 'beam_plugins': None, 'save_main_session': False, 'sdk_location': 'default', 'extra_packages': None, 'prebuild_sdk_container_engine': None, 'prebuild_sdk_container_base_image': None, 'docker_registry_push_url': None, 'job_endpoint': None, 'artifact_endpoint': None, 'job_server_timeout': 60, 'environment_type': None, 'environment_config': None, 'environment_options': None, 'sdk_worker_parallelism': 1, 'environment_cache_millis': 0, 'output_executable_path': None, 'artifacts_dir': None, 'job_port': 0, 'artifact_port': 0, 'expansion_port': 0, 'flink_master': '[auto]', 'flink_version': '1.10', 'flink_job_server_jar': None, 'flink_submit_uber_jar': False, 'spark_master_url': 'local[4]', 'spark_job_server_jar': None, 'spark_submit_uber_jar': False, 'spark_rest_url': None, 'on_success_matcher': None, 'dry_run': False, 'wait_until_finish_duration': None, 'pubsubRootUrl': None, 's3_access_key_id': None, 's3_secret_access_key': None, 's3_session_token': None, 's3_endpoint_url': None, 's3_region_name': None, 's3_api_version': None, 's3_verify': None, 's3_disable_ssl': False}\n",
      "\n",
      "Found dataset /tmp/fids-capstone-data/data/consultant-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-targetvideo-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-utterance-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-utterance-targetvideo-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-utterance-token-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/ncslgr-corpus-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-targetvideo-utterance-token-frame-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/vocabulary-index.csv\n",
      "Beam PL: ALL DONE!\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/tmp/fids-capstone-data/data\"\n",
    "\n",
    "data_extractor.run(max_target_videos=-1, data_dir=data_dir, use_beam=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PipelineOptions:\n{'runner': 'InteractiveRunner', 'streaming': False, 'beam_services': {}, 'type_check_strictness': 'DEFAULT_TO_ANY', 'type_check_additional': '', 'pipeline_type_check': True, 'runtime_type_check': False, 'performance_runtime_type_check': False, 'direct_runner_use_stacked_bundle': True, 'direct_runner_bundle_repeat': 0, 'direct_num_workers': 0, 'direct_running_mode': 'multi_threading', 'dataflow_endpoint': 'https://dataflow.googleapis.com', 'project': 'my-project', 'job_name': None, 'staging_location': None, 'temp_location': None, 'region': None, 'service_account_email': None, 'no_auth': False, 'template_location': None, 'labels': None, 'update': False, 'transform_name_mapping': None, 'enable_streaming_engine': False, 'dataflow_kms_key': None, 'flexrs_goal': None, 'hdfs_host': None, 'hdfs_port': None, 'hdfs_user': None, 'hdfs_full_urls': False, 'num_workers': None, 'max_num_workers': None, 'autoscaling_algorithm': None, 'machine_type': None, 'disk_size_gb': None, 'disk_type': None, 'worker_region': None, 'worker_zone': None, 'zone': None, 'network': None, 'subnetwork': None, 'worker_harness_container_image': None, 'sdk_harness_container_image_overrides': None, 'use_public_ips': None, 'min_cpu_platform': None, 'dataflow_worker_jar': None, 'dataflow_job_file': None, 'experiments': None, 'number_of_worker_harness_threads': None, 'profile_cpu': False, 'profile_memory': False, 'profile_location': None, 'profile_sample_rate': 1.0, 'requirements_file': None, 'requirements_cache': None, 'setup_file': None, 'beam_plugins': None, 'save_main_session': False, 'sdk_location': 'default', 'extra_packages': None, 'prebuild_sdk_container_engine': None, 'prebuild_sdk_container_base_image': None, 'docker_registry_push_url': None, 'job_endpoint': None, 'artifact_endpoint': None, 'job_server_timeout': 60, 'environment_type': None, 'environment_config': None, 'environment_options': None, 'sdk_worker_parallelism': 1, 'environment_cache_millis': 0, 'output_executable_path': None, 'artifacts_dir': None, 'job_port': 0, 'artifact_port': 0, 'expansion_port': 0, 'flink_master': '[auto]', 'flink_version': '1.10', 'flink_job_server_jar': None, 'flink_submit_uber_jar': False, 'spark_master_url': 'local[4]', 'spark_job_server_jar': None, 'spark_submit_uber_jar': False, 'spark_rest_url': None, 'on_success_matcher': None, 'dry_run': False, 'wait_until_finish_duration': None, 'pubsubRootUrl': None, 's3_access_key_id': None, 's3_secret_access_key': None, 's3_session_token': None, 's3_endpoint_url': None, 's3_region_name': None, 's3_api_version': None, 's3_verify': None, 's3_disable_ssl': False}\n\n"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "    'project': 'my-project', # change\n",
    "    # 'runner': 'DirectRunner',\n",
    "    'runner': 'InteractiveRunner',\n",
    "    'direct_num_workers': 0, # 0 is use all available cores\n",
    "    'direct_running_mode': 'multi_threading', # ['in_memory', 'multi_threading', 'multi_processing'] # 'multi_processing' doesn't seem to work for DirectRunner?\n",
    "    'streaming': False # set to True if data source is unbounded (e.g. GCP PubSub)\n",
    "}\n",
    "pipeline_options = PipelineOptions(flags=[], **options) # easier to pass in options from command-line this way\n",
    "print(f\"PipelineOptions:\\n{pipeline_options.get_all_options()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidscs_globals.DATA_ROOT_DIR = data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "can_proceed = True\n",
    "\n",
    "if not tf.io.gfile.exists(fidscs_globals.DATA_ROOT_DIR) or len(tf.io.gfile.listdir(fidscs_globals.DATA_ROOT_DIR))==0:\n",
    "    print(f\"{fidscs_globals.VALIDATION_FATAL_ERROR_TEXT} data directory does not exist or is empty!\")\n",
    "    can_proceed = False\n",
    "else:\n",
    "    fidscs_globals.VIDEO_DIR = os.path.join(fidscs_globals.DATA_ROOT_DIR, 'videos')\n",
    "    fidscs_globals.STICHED_VIDEO_FRAMES_DIR = os.path.join(fidscs_globals.DATA_ROOT_DIR, 'stitched_video_frames')\n",
    "    fidscs_globals.CORPUS_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.CORPUS_DS_FNAME)\n",
    "    fidscs_globals.DOCUMENT_ASL_CONSULTANT_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.DOCUMENT_ASL_CONSULTANT_DS_FNAME)\n",
    "    fidscs_globals.ASL_CONSULTANT_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.ASL_CONSULTANT_DS_FNAME)\n",
    "    fidscs_globals.VIDEO_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.VIDEO_DS_FNAME)\n",
    "    fidscs_globals.VIDEO_SEGMENT_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.VIDEO_SEGMENT_DS_FNAME)\n",
    "    fidscs_globals.VIDEO_FRAME_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.VIDEO_FRAME_DS_FNAME)\n",
    "    fidscs_globals.UTTERANCE_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.UTTERANCE_DS_FNAME)\n",
    "    fidscs_globals.UTTERANCE_VIDEO_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.UTTERANCE_VIDEO_DS_FNAME)\n",
    "    fidscs_globals.UTTERANCE_TOKEN_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.UTTERANCE_TOKEN_DS_FNAME)\n",
    "    fidscs_globals.UTTERANCE_TOKEN_FRAME_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.UTTERANCE_TOKEN_FRAME_DS_FNAME)\n",
    "    fidscs_globals.VOCABULARY_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.VOCABULARY_DS_FNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "pl = beam.Pipeline(options=pipeline_options)\n",
    "\n",
    "full_target_vid_index_schemad_pcoll = beam__common.pl__1__read_target_vid_index_csv(pl)\n",
    "corpus_index_schemad_pcoll = beam__common.pl__1__read_corpus_index_csv(pl) # XML is base-64 encode but we no longer need it (to decode it) since it is only used to create the datasets\n",
    "# corpus_index_decoded_XML_pcoll = pl__2__decode_XML(corpus_index_schemad_pcoll) # see above\n",
    "\n",
    "asl_consultant_index_schemad_pcoll = beam__common.pl__1__read_asl_consultant_index_csv(pl)\n",
    "document_asl_consultant_utterance_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_utterance_index_csv(pl)\n",
    "document_asl_consultant_target_video_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_target_video_index_csv(pl)\n",
    "document_asl_consultant_utterance_video_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_utterance_video_index_csv(pl)\n",
    "document_target_video_segment_index_schemad_pcoll = beam__common.pl__1__read_document_target_video_segment_index_csv(pl)\n",
    "vocabulary_index_schemad_pcoll = beam__common.pl__1__read_vocabulary_index_csv(pl)\n",
    "document_asl_consultant_utterance_token_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_utterance_token_index_csv(pl)\n",
    "document_asl_consultant_target_video_frame_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_target_video_frame_index_csv(pl)\n",
    "document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_target_video_utterance_token_frame_index_csv(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll is the main table we use for training.\n",
    "#     This will ultimately provide which frame sequences correspond to individual tokens.\n",
    "\n",
    "# But our first measure is to build train and validation sets (for tokens).\n",
    "#   In order to split up train vs validation sets, we need to compare \"apples to apples\".\n",
    "#   That is, in order for a token (TokenID) to be considered a candidate for the split,\n",
    "#   we require at least two of the same (TokenID, CameraPerspective) wherein the ASL\n",
    "#   consultant for each differs.  We would prefer more than two of these tuples, each\n",
    "#   having unique ASL consultants in the set of occurrences, with the majority of said\n",
    "#   tuples being assigned to the training set and the remainder (at least one) being\n",
    "#   assigned to the validation set.  We would like to achieve a 90/10 split, ideally,\n",
    "#   but we will take what we get.\n",
    "\n",
    "# document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll:\n",
    "    # beam.Row(\n",
    "    #   DocumentID=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[0]]),\n",
    "    #   ASLConsultantID=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[1]]),\n",
    "    #   CameraPerspective=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[2]]),\n",
    "    #   TargetVideoFilename=str(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[3]]),\n",
    "    #   UtteranceSequence=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[4]]),\n",
    "    #   TokenSequence=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[5]]),\n",
    "    #   FrameSequence=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[6]]),\n",
    "    #   TokenID=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[7]])\n",
    "    # )\n",
    "\n",
    "# We will transform this into tuples of the form:\n",
    "    # [\n",
    "    #     'TokenID', \n",
    "    #     'CameraPerspective', \n",
    "    #     'DocumentID', \n",
    "    #     'ASLConsultantID', \n",
    "    #     'TargetVideoFilename', \n",
    "    #     'UtteranceSequence', \n",
    "    #     'TokenSequence',\n",
    "\n",
    "    #     'FrameSequence'\n",
    "    # ]\n",
    "\n",
    "dctvustsfs = (\n",
    "    document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll\n",
    "    | \"Beam PL: extract (TokenID,CameraPerspective,ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence,FrameSequence) from dctvustsfs schemad pcoll\" >> beam.Map(\n",
    "            lambda dctvustsfs_row: (\n",
    "                dctvustsfs_row.TokenID,\n",
    "                dctvustsfs_row.CameraPerspective,\n",
    "                dctvustsfs_row.ASLConsultantID,\n",
    "                dctvustsfs_row.TargetVideoFilename,\n",
    "                dctvustsfs_row.UtteranceSequence,\n",
    "                dctvustsfs_row.TokenSequence,\n",
    "                dctvustsfs_row.FrameSequence\n",
    "            )\n",
    "        )\n",
    ")\n",
    "\n",
    "# for train-validation split, we want to key/group by (TokenID, CameraPerspective) with lists of unique (ASLConsultantID, TargetVideoFilename, UtteranceSequence, TokenSequence) > 1\n",
    "ctvusts_by_tcp = (\n",
    "    dctvustsfs\n",
    "    | \"Beam PL: extract ((TokenID,CameraPerspective), (ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence)) from dctvustsfs\" >> beam.Map(\n",
    "            lambda dctvustsfs_row_tpl: (\n",
    "                (\n",
    "                    dctvustsfs_row_tpl[0],\n",
    "                    dctvustsfs_row_tpl[1]\n",
    "                ),\n",
    "                (\n",
    "                    dctvustsfs_row_tpl[2],\n",
    "                    dctvustsfs_row_tpl[3],\n",
    "                    dctvustsfs_row_tpl[4],\n",
    "                    dctvustsfs_row_tpl[5]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    | \"Beam PL: select distinct ((TokenID,CameraPerspective), (ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence)) from ctvusts_by_tcp\" >> beam.Distinct()\n",
    "    | \"Beam PL: group (ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence) by key (TokenID,CameraPerspective)\" >> beam.GroupByKey() \n",
    "    # the above produces tuples of the form:\n",
    "        # (\n",
    "        #     (\n",
    "        #         TokenID,\n",
    "        #         CameraPerspective\n",
    "        #     ),\n",
    "        #     listof(\n",
    "        #       (ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence)\n",
    "        #     )\n",
    "        # )\n",
    ")\n",
    "\n",
    "\n",
    "def flatten_ctvusts_by_tcp(ctvusts_by_tcp_tpl):\n",
    "    return [\n",
    "        (\n",
    "            ctvusts_by_tcp_tpl[0][0],   # TokenID\n",
    "            ctvusts_by_tcp_tpl[0][1],   # CameraPerspective\n",
    "            ctvusts_tpl[0],             # ASLConsultantID\n",
    "            ctvusts_tpl[1],             # TargetVideoFilename\n",
    "            ctvusts_tpl[2],             # UtteranceSequence\n",
    "            ctvusts_tpl[3]              # TokenSequence\n",
    "        ) for ctvusts_tpl in ctvusts_by_tcp_tpl[1]\n",
    "    ]\n",
    "\n",
    "ctvusts_by_tcp__gt_1 = (\n",
    "    ctvusts_by_tcp\n",
    "    | \"Beam PL: filter candidate (TokenID,CameraPerspective) for test-validation split\" >> beam.Filter(\n",
    "            lambda list_ctvusts_by_tcp_tpl: len(set(list_ctvusts_by_tcp_tpl[1])) > 1\n",
    "        )\n",
    "    | \"Beam PL: flatten filtered (TokenID,CameraPerspective) candidates for test-validation split\" >> beam.FlatMap(flatten_ctvusts_by_tcp)\n",
    ")\n",
    "\n",
    "ctvusts_by_tcp__lte_1 = (\n",
    "    ctvusts_by_tcp\n",
    "    | \"Beam PL: filter non-candidate (TokenID,CameraPerspective) for test-validation split\" >> beam.Filter(\n",
    "            lambda list_ctvusts_by_tcp_tpl: len(set(list_ctvusts_by_tcp_tpl[1])) <= 1\n",
    "        )\n",
    "    | \"Beam PL: flatten filtered (TokenID,CameraPerspective) non-candidates for test-validation split\" >> beam.FlatMap(flatten_ctvusts_by_tcp)\n",
    ")"
   ]
  },
  {
   "source": [
    "<p><br>\n",
    "\n",
    "#### Finally, execute validation/train split on ctvusts_by_tcp__gt_1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to put ctvusts_by_tcp__gt_1 back into ((TokenID, CameraPerspective), (ASLConsultantID, TargetVideoFilename, UtteranceSequence, TokenSequence)) form\n",
    "def rekey_ctvusts_by_tcp(ctvusts_by_tcp_tpl):\n",
    "    return (\n",
    "        (\n",
    "            ctvusts_by_tcp_tpl[0],  # TokenID\n",
    "            ctvusts_by_tcp_tpl[1]   # CameraPerspective\n",
    "        ),\n",
    "        (\n",
    "            ctvusts_by_tcp_tpl[2],  # ASLConsultantID\n",
    "            ctvusts_by_tcp_tpl[3],  # TargetVideoFilename\n",
    "            ctvusts_by_tcp_tpl[4],  # UtteranceSequence\n",
    "            ctvusts_by_tcp_tpl[5]   # TokenSequence\n",
    "        )\n",
    "    )\n",
    "\n",
    "def val_train_split__ctvusts_by_tcp__gt_1__tpl(ctvusts_list__by__tcp__gt_1__tpl):\n",
    "    \"\"\"\n",
    "    ctvusts_list__by__tcp__gt_1__tpl\n",
    "        (\n",
    "            (TokenID,CameraPerspective), # key\n",
    "            listof(\n",
    "                (ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence)\n",
    "            )\n",
    "        )\n",
    "    \"\"\"\n",
    "    ctvusts_list = ctvusts_list__by__tcp__gt_1__tpl[1].copy() # we need a copy since we want to shuffle\n",
    "    random.shuffle(ctvusts_list)\n",
    "    len_ctvusts_list = len(ctvusts_list)\n",
    "    val_len_ctvusts_list = int(len_ctvusts_list*fidscs_globals.VALIDATION_SIZE_RATIO) if len_ctvusts_list > int(((1-fidscs_globals.VALIDATION_SIZE_RATIO)*100)/10) else 1\n",
    "    train__ctvusts_list, val__ctvusts_list = ctvusts_list[val_len_ctvusts_list:], ctvusts_list[:val_len_ctvusts_list]\n",
    "    return (\n",
    "        (\n",
    "            ctvusts_list__by__tcp__gt_1__tpl[0][0],    # TokenID\n",
    "            ctvusts_list__by__tcp__gt_1__tpl[0][1]     # CameraPerspective\n",
    "        ),\n",
    "        (\n",
    "            train__ctvusts_list,\n",
    "            val__ctvusts_list\n",
    "        )\n",
    "    )\n",
    "\n",
    "val_train_split_basis__ctvusts_by_tcp__gt_1 = (\n",
    "    ctvusts_by_tcp__gt_1\n",
    "    | \"Beam PL: rekey ctvusts_by_tcp__gt_1 for validation/train split\" >> beam.Map(rekey_ctvusts_by_tcp)\n",
    "    | \"Beam PL: group (ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence) rekeyed by (TokenID,CameraPerspective)\" >> beam.GroupByKey()\n",
    "    # the above produces tuples of the form:\n",
    "        # (\n",
    "        #     (TokenID,CameraPerspective), # key\n",
    "        #     listof(\n",
    "        #       (ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence)\n",
    "        #     )\n",
    "        # )\n",
    "    | \"Beam PL: split rekeyed ctvusts_list_by_tcp__gt_1\" >> beam.Map(val_train_split__ctvusts_by_tcp__gt_1__tpl)\n",
    "    # the above produces tuples of the form:\n",
    "        # (\n",
    "        #     (TokenID,CameraPerspective), # key\n",
    "        #     (\n",
    "        #       test_list_of(ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence),\n",
    "        #       val_list_of(ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence),\n",
    "        #     )\n",
    "        # )\n",
    ")\n",
    "\n",
    "train__ctvusts_by_tcp__gt_1 = (\n",
    "    val_train_split_basis__ctvusts_by_tcp__gt_1\n",
    "    | \"Beam PL: select train sublist from val_train_split_basis__ctvusts_by_tcp__gt_1\" >> beam.Map(\n",
    "            lambda val_train_split_basis__ctvusts_by_tcp__gt_1_tpl: [\n",
    "                (\n",
    "                    val_train_split_basis__ctvusts_by_tcp__gt_1_tpl[0][0],  # TokenID\n",
    "                    val_train_split_basis__ctvusts_by_tcp__gt_1_tpl[0][1],  # CameraPerspective\n",
    "                    train_ctvusts_tpl[0],                                   # ASLConsultantID\n",
    "                    train_ctvusts_tpl[1],                                   # TargetVideoFilename\n",
    "                    train_ctvusts_tpl[2],                                   # UtteranceSequence\n",
    "                    train_ctvusts_tpl[3]                                    # TokenSequence\n",
    "                ) for train_ctvusts_tpl in val_train_split_basis__ctvusts_by_tcp__gt_1_tpl[1][0] # index [1][0] points to train sublist\n",
    "            ]\n",
    "        )\n",
    "    | \"Beam PL: 'explode list_train__ctvusts_by_tcp__gt_1_tpl\" >> beam.FlatMap(lambda list_train__ctvusts_by_tcp__gt_1_tpl: list_train__ctvusts_by_tcp__gt_1_tpl)\n",
    ")\n",
    "\n",
    "val__ctvusts_by_tcp__gt_1 = (\n",
    "    val_train_split_basis__ctvusts_by_tcp__gt_1\n",
    "    | \"Beam PL: select validation sublist from val_train_split_basis__ctvusts_by_tcp__gt_1\" >> beam.Map(\n",
    "            lambda val_train_split_basis__ctvusts_by_tcp__gt_1_tpl: [\n",
    "                (\n",
    "                    val_train_split_basis__ctvusts_by_tcp__gt_1_tpl[0][0],  # TokenID\n",
    "                    val_train_split_basis__ctvusts_by_tcp__gt_1_tpl[0][1],  # CameraPerspective\n",
    "                    val_ctvusts_tpl[0],                                     # ASLConsultantID\n",
    "                    val_ctvusts_tpl[1],                                     # TargetVideoFilename\n",
    "                    val_ctvusts_tpl[2],                                     # UtteranceSequence\n",
    "                    val_ctvusts_tpl[3]                                      # TokenSequence\n",
    "                ) for val_ctvusts_tpl in val_train_split_basis__ctvusts_by_tcp__gt_1_tpl[1][1] # index [1][1] points to validation sublist\n",
    "            ]\n",
    "        )\n",
    "    | \"Beam PL: 'explode list_val__ctvusts_by_tcp__gt_1_tpl\" >> beam.FlatMap(lambda list_val__ctvusts_by_tcp__gt_1_tpl: list_val__ctvusts_by_tcp__gt_1_tpl)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join train__ctvusts_by_tcp__gt_1 to dctvustsfs\n",
    "train__ctvusts_by_tcp__gt_1__keys = (\n",
    "    train__ctvusts_by_tcp__gt_1\n",
    "    | \"Beam PL: extract ((TokenID,CameraPerspective,ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence), '<train__ctvusts_by_tcp__gt_1__has_key>') for join to dctvustsfs\" >> beam.Map(\n",
    "            lambda train__ctvusts_by_tcp__gt_1_tpl : (\n",
    "                (\n",
    "                    train__ctvusts_by_tcp__gt_1_tpl[0], # TokenID\n",
    "                    train__ctvusts_by_tcp__gt_1_tpl[1], # CameraPerspective\n",
    "                    train__ctvusts_by_tcp__gt_1_tpl[2], # ASLConsultantID\n",
    "                    train__ctvusts_by_tcp__gt_1_tpl[3], # TargetVideoFilename\n",
    "                    train__ctvusts_by_tcp__gt_1_tpl[4], # UtteranceSequence\n",
    "                    train__ctvusts_by_tcp__gt_1_tpl[5]  # TokenSequence\n",
    "                ),\n",
    "                \"<train__ctvusts_by_tcp__gt_1__has_key>\"\n",
    "            )\n",
    "        )\n",
    ")\n",
    "\n",
    "dctvustsfs__frame_sequences = (\n",
    "    dctvustsfs\n",
    "    | \"Beam PL: extract ((TokenID,CameraPerspective,ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence), FrameSequence) for join to train__ctvusts_by_tcp__gt_1/val__ctvusts_by_tcp__gt_1\" >> beam.Map(\n",
    "            lambda dctvustsfs_tpl: (\n",
    "                (\n",
    "                    dctvustsfs_tpl[0],  # TokenID\n",
    "                    dctvustsfs_tpl[1],  # CameraPerspective\n",
    "                    dctvustsfs_tpl[2],  # ASLConsultantID\n",
    "                    dctvustsfs_tpl[3],  # TargetVideoFilename\n",
    "                    dctvustsfs_tpl[4],  # UtteranceSequence\n",
    "                    dctvustsfs_tpl[5]   # TokenSequence\n",
    "                ),\n",
    "                dctvustsfs_tpl[6]       # FrameSequence\n",
    "            )\n",
    "        )\n",
    ")\n",
    "\n",
    "train_dctvustsfs__gt__1 = (\n",
    "    ({\n",
    "      'has_key': train__ctvusts_by_tcp__gt_1__keys,\n",
    "      'frame_sequences': dctvustsfs__frame_sequences\n",
    "    })\n",
    "    | \"Beam PL: join train__ctvusts_by_tcp__gt_1 to dctvustsfs\" >> beam.CoGroupByKey()\n",
    "    # the above produces tuples of the form:\n",
    "        # (\n",
    "        #     (\n",
    "        #         <TokenID>,\n",
    "        #         <CameraPerspective>,\n",
    "        #         <ASLConsultantID>,\n",
    "        #         <TargetVideoFilename>,\n",
    "        #         <UtteranceSequence>,\n",
    "        #         <TokenSequence>\n",
    "        #     ),\n",
    "        #     {\n",
    "        #         'has_key': listof('<train__ctvusts_by_tcp__gt_1__has_key>'),    # should have only one/single element\n",
    "        #         'frame_sequences': listof(<FrameSequence>)                      # many\n",
    "        #     }\n",
    "        # )\n",
    "    | \"Beam PL: filter out mismatches from joined train__ctvusts_by_tcp__gt_1 to dctvustsfs\" >> beam.Filter(\n",
    "            lambda joined__train__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl: \n",
    "                len(joined__train__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[1]['has_key'])>0 and \\\n",
    "                    len(joined__train__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[1]['frame_sequences'])>0\n",
    "        )\n",
    "    | \"Beam PL: 'explode' listof(<FrameSequence>) from joined train__ctvusts_by_tcp__gt_1 to dctvustsfs to list of tuples\" >> beam.Map(\n",
    "            lambda joined__train__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl: [\n",
    "                (\n",
    "                    joined__train__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[0][0], # TokenID\n",
    "                    joined__train__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[0][1], # CameraPerspective\n",
    "                    joined__train__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[0][2], # ASLConsultantID\n",
    "                    joined__train__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[0][3], # TargetVideoFilename\n",
    "                    joined__train__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[0][4], # UtteranceSequence\n",
    "                    joined__train__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[0][5], # TokenSequence\n",
    "                    frame_seq\n",
    "                ) for frame_seq in sorted(joined__train__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[1]['frame_sequences'])\n",
    "            ]\n",
    "        )\n",
    "    | \"Beam PL: 'explode' listof((TokenID,CameraPerspective,ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence, FrameSequence)) from joined train__ctvusts_by_tcp__gt_1 to dctvustsfs\" >> beam.FlatMap(\n",
    "            lambda list_joined__train__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl: list_joined__train__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join val__ctvusts_by_tcp__gt_1 to dctvustsfs\n",
    "val__ctvusts_by_tcp__gt_1__keys = (\n",
    "    val__ctvusts_by_tcp__gt_1\n",
    "    | \"Beam PL: extract ((TokenID,CameraPerspective,ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence), '<val__ctvusts_by_tcp__gt_1__has_key>') for join to dctvustsfs\" >> beam.Map(\n",
    "            lambda val__ctvusts_by_tcp__gt_1_tpl : (\n",
    "                (\n",
    "                    val__ctvusts_by_tcp__gt_1_tpl[0], # TokenID\n",
    "                    val__ctvusts_by_tcp__gt_1_tpl[1], # CameraPerspective\n",
    "                    val__ctvusts_by_tcp__gt_1_tpl[2], # ASLConsultantID\n",
    "                    val__ctvusts_by_tcp__gt_1_tpl[3], # TargetVideoFilename\n",
    "                    val__ctvusts_by_tcp__gt_1_tpl[4], # UtteranceSequence\n",
    "                    val__ctvusts_by_tcp__gt_1_tpl[5]  # TokenSequence\n",
    "                ),\n",
    "                \"<val__ctvusts_by_tcp__gt_1__has_key>\"\n",
    "            )\n",
    "        )\n",
    ")\n",
    "\n",
    "val_dctvustsfs__gt__1 = (\n",
    "    ({\n",
    "      'has_key': val__ctvusts_by_tcp__gt_1__keys,\n",
    "      'frame_sequences': dctvustsfs__frame_sequences\n",
    "    })\n",
    "    | \"Beam PL: join val__ctvusts_by_tcp__gt_1 to dctvustsfs\" >> beam.CoGroupByKey()\n",
    "    # the above produces tuples of the form:\n",
    "        # (\n",
    "        #     (\n",
    "        #         <TokenID>,\n",
    "        #         <CameraPerspective>,\n",
    "        #         <ASLConsultantID>,\n",
    "        #         <TargetVideoFilename>,\n",
    "        #         <UtteranceSequence>,\n",
    "        #         <TokenSequence>\n",
    "        #     ),\n",
    "        #     {\n",
    "        #         'has_key': listof('<val__ctvusts_by_tcp__gt_1__has_key>'),    # should have only one/single element\n",
    "        #         'frame_sequences': listof(<FrameSequence>)                      # many\n",
    "        #     }\n",
    "        # )\n",
    "    | \"Beam PL: filter out mismatches from joined val__ctvusts_by_tcp__gt_1 to dctvustsfs\" >> beam.Filter(\n",
    "            lambda joined__val__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl: \n",
    "                len(joined__val__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[1]['has_key'])>0 and \\\n",
    "                    len(joined__val__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[1]['frame_sequences'])>0\n",
    "        )\n",
    "    | \"Beam PL: 'explode' listof(<FrameSequence>) from joined val__ctvusts_by_tcp__gt_1 to dctvustsfs to list of tuples\" >> beam.Map(\n",
    "            lambda joined__val__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl: [\n",
    "                (\n",
    "                    joined__val__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[0][0],   # TokenID\n",
    "                    joined__val__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[0][1],   # CameraPerspective\n",
    "                    joined__val__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[0][2],   # ASLConsultantID\n",
    "                    joined__val__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[0][3],   # TargetVideoFilename\n",
    "                    joined__val__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[0][4],   # UtteranceSequence\n",
    "                    joined__val__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[0][5],   # TokenSequence\n",
    "                    frame_seq                                                       # FrameSequence\n",
    "                ) for frame_seq in sorted(joined__val__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl[1]['frame_sequences'])\n",
    "            ]\n",
    "        )\n",
    "    | \"Beam PL: 'explode' listof((TokenID,CameraPerspective,ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence, FrameSequence)) from joined val__ctvusts_by_tcp__gt_1 to dctvustsfs\" >> beam.FlatMap(\n",
    "            lambda list_joined__val__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl: list_joined__val__ctvusts_by_tcp__gt_1__to__dctvustsfs__tpl\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train__ctvusts_by_tcp__lte_1__keys = (\n",
    "    ctvusts_by_tcp__lte_1\n",
    "    | \"Beam PL: extract ((TokenID,CameraPerspective,ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence), '<ctvusts_by_tcp__lte_1_tpl__has_key>') for join to dctvustsfs\" >> beam.Map(\n",
    "            lambda ctvusts_by_tcp__lte_1_tpl : (\n",
    "                (\n",
    "                    ctvusts_by_tcp__lte_1_tpl[0], # TokenID\n",
    "                    ctvusts_by_tcp__lte_1_tpl[1], # CameraPerspective\n",
    "                    ctvusts_by_tcp__lte_1_tpl[2], # ASLConsultantID\n",
    "                    ctvusts_by_tcp__lte_1_tpl[3], # TargetVideoFilename\n",
    "                    ctvusts_by_tcp__lte_1_tpl[4], # UtteranceSequence\n",
    "                    ctvusts_by_tcp__lte_1_tpl[5]  # TokenSequence\n",
    "                ),\n",
    "                \"<ctvusts_by_tcp__lte_1_tpl__has_key>\"\n",
    "            )\n",
    "        )\n",
    ")\n",
    "\n",
    "train_dctvustsfs__lte_1 = (\n",
    "    ({\n",
    "      'has_key': train__ctvusts_by_tcp__lte_1__keys,\n",
    "      'frame_sequences': dctvustsfs__frame_sequences\n",
    "    })\n",
    "    | \"Beam PL: join ctvusts_by_tcp__lte_1 to dctvustsfs\" >> beam.CoGroupByKey()\n",
    "    # the above produces tuples of the form:\n",
    "        # (\n",
    "        #     (\n",
    "        #         <TokenID>,\n",
    "        #         <CameraPerspective>,\n",
    "        #         <ASLConsultantID>,\n",
    "        #         <TargetVideoFilename>,\n",
    "        #         <UtteranceSequence>,\n",
    "        #         <TokenSequence>\n",
    "        #     ),\n",
    "        #     {\n",
    "        #         'has_key': listof('<ctvusts_by_tcp__lte_1_tpl__has_key>'),    # should have only one/single element\n",
    "        #         'frame_sequences': listof(<FrameSequence>)                      # many\n",
    "        #     }\n",
    "        # )\n",
    "    | \"Beam PL: filter out mismatches from joined train__ctvusts_by_tcp__lte_1 to dctvustsfs\" >> beam.Filter(\n",
    "            lambda joined__train__ctvusts_by_tcp__lte_1__to__dctvustsfs__tpl: \n",
    "                len(joined__train__ctvusts_by_tcp__lte_1__to__dctvustsfs__tpl[1]['has_key'])>0 and \\\n",
    "                    len(joined__train__ctvusts_by_tcp__lte_1__to__dctvustsfs__tpl[1]['frame_sequences'])>0\n",
    "        )\n",
    "    | \"Beam PL: 'explode' listof(<FrameSequence>) from joined train__ctvusts_by_tcp__lte_1 to dctvustsfs to list of tuples\" >> beam.Map(\n",
    "            lambda joined__train__ctvusts_by_tcp__lte_1__to__dctvustsfs__tpl: [\n",
    "                (\n",
    "                    joined__train__ctvusts_by_tcp__lte_1__to__dctvustsfs__tpl[0][0], # TokenID\n",
    "                    joined__train__ctvusts_by_tcp__lte_1__to__dctvustsfs__tpl[0][1], # CameraPerspective\n",
    "                    joined__train__ctvusts_by_tcp__lte_1__to__dctvustsfs__tpl[0][2], # ASLConsultantID\n",
    "                    joined__train__ctvusts_by_tcp__lte_1__to__dctvustsfs__tpl[0][3], # TargetVideoFilename\n",
    "                    joined__train__ctvusts_by_tcp__lte_1__to__dctvustsfs__tpl[0][4], # UtteranceSequence\n",
    "                    joined__train__ctvusts_by_tcp__lte_1__to__dctvustsfs__tpl[0][5], # TokenSequence\n",
    "                    frame_seq\n",
    "                ) for frame_seq in sorted(joined__train__ctvusts_by_tcp__lte_1__to__dctvustsfs__tpl[1]['frame_sequences'])\n",
    "            ]\n",
    "        )\n",
    "    | \"Beam PL: 'explode' listof((TokenID,CameraPerspective,ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence, FrameSequence)) from joined ttrain__ctvusts_by_tcp__lte_1 to dctvustsfs\" >> beam.FlatMap(\n",
    "            lambda list_joined__train__ctvusts_by_tcp__lte_1__to__dctvustsfs__tpl: list_joined__train__ctvusts_by_tcp__lte_1__to__dctvustsfs__tpl\n",
    "        )\n",
    ")\n",
    "\n",
    "train_dctvustsfs__all = (\n",
    "    (train_dctvustsfs__gt__1, train_dctvustsfs__lte_1) \n",
    "    | f\"Beam PL: merge train_dctvustsfs__gt__1 with train_dctvustsfs__lte_1\" >> beam.Flatten() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all COMPLETE utterances that can be formed with token-cameraperspective pairs from the validation set\n",
    "\n",
    "val_tcp__gt__1 = (\n",
    "    val_dctvustsfs__gt__1\n",
    "    | \"Beam PL: extract (TokenID, CameraPerspective) from val_dctvustsfs__gt__1\" >> beam.Map(\n",
    "            lambda tpl: (\n",
    "                tpl[0], # TokenID\n",
    "                tpl[1]  # CameraPerspective\n",
    "            )\n",
    "        )\n",
    "    | \"Beam PL: select distinct (TokenID, CameraPerspective) from val_dctvustsfs__gt__1\" >> beam.Distinct()\n",
    ")\n",
    "\n",
    "complete_utterances__with__val_tcp__gt__1 = (\n",
    "    dctvustsfs\n",
    "    | \"Beam PL: extract (ASLConsultantID,TargetVideoFilename,CameraPerspective,UtteranceSequence,TokenSequence,TokenID) from dctvustsfs\" >> beam.Map(\n",
    "            lambda tpl: (\n",
    "                tpl[2], # <ASLConsultantID>\n",
    "                tpl[3], # <TargetVideoFilename>\n",
    "                tpl[4], # <UtteranceSequence>\n",
    "                tpl[1], # <CameraPerspective>\n",
    "\n",
    "                tpl[5], # <TokenSequence>\n",
    "                tpl[0]  # <TokenID>\n",
    "            )\n",
    "        )\n",
    "    | \"Beam PL: select distinct (ASLConsultantID,TargetVideoFilename,CameraPerspective,UtteranceSequence,TokenSequence,TokenID) from dctvustsfs\" >> beam.Distinct()\n",
    "    | \"Beam PL: transform distinct ctvcpustst tuples to tst_by_ctvuscp\" >> beam.Map(\n",
    "            lambda tpl: (\n",
    "                (\n",
    "                    tpl[0], # <ASLConsultantID>\n",
    "                    tpl[1], # <TargetVideoFilename>\n",
    "                    tpl[2], # <UtteranceSequence>\n",
    "                    tpl[3]  # <CameraPerspective>\n",
    "                ),\n",
    "                (\n",
    "                    tpl[4], # <TokenSequence>\n",
    "                    tpl[5]  # <TokenID>\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    | \"Beam PL: collect list of tokenseq-tokenid for each (<ASLConsultantID>, <TargetVideoFilename>, <UtteranceSequence>, <CameraPerspective>)\" >> beam.GroupByKey()\n",
    "    # the above produces tuples of the form:\n",
    "        # (\n",
    "        #     (<ASLConsultantID>,<TargetVideoFilename>,<UtteranceSequence>,<CameraPerspective>), # key\n",
    "        #     listof((<TokenSequence>,<TokenID>))\n",
    "        # )\n",
    "    | \"Beam PL: sort list of tokenseq-tokenid by tokenseq for each (<ASLConsultantID>, <TargetVideoFilename>, <UtteranceSequence>, <CameraPerspective>)\" >> beam.Map(\n",
    "            lambda tpl: (\n",
    "                (\n",
    "                    tpl[0][0], # <ASLConsultantID>\n",
    "                    tpl[0][1], # <TargetVideoFilename>\n",
    "                    tpl[0][2], # <UtteranceSequence>\n",
    "                    tpl[0][3]  # <CameraPerspective>\n",
    "                ),\n",
    "                [(tst_tpl[1], tpl[0][3]) for tst_tpl in sorted(tpl[1], key=lambda tst_tpl: tst_tpl[0])]\n",
    "            )\n",
    "        )\n",
    "    # the above produces tuples of the form:\n",
    "        # (\n",
    "        #     (<ASLConsultantID>,<TargetVideoFilename>,<UtteranceSequence>,<CameraPerspective>), # key\n",
    "        #     listof((<TokenID>, <CameraPerspective>)) # sorted by <TokenSequence>\n",
    "        # )\n",
    "\n",
    "    # now we need to filter all of the above (<ASLConsultantID>,<TargetVideoFilename>,<UtteranceSequence>,<CameraPerspective>) where every (<TokenID>, <CameraPerspective>) in the corresponding list exists in val_tcp__gt__1\n",
    "    | \"Beam PL: filter matching rows from vid index\" >> beam.Filter(\n",
    "        lambda list_tcp_tpl__by__ctvuscp__tpl, existing_val_tcp_tpls: all(tcp_tpl in existing_val_tcp_tpls for tcp_tpl in list_tcp_tpl__by__ctvuscp__tpl[1]),\n",
    "        existing_val_tcp_tpls=beam.pvalue.AsIter(val_tcp__gt__1)\n",
    "      )\n",
    "    | \"Beam PL: extract (<ASLConsultantID>,<TargetVideoFilename>,<UtteranceSequence>,<CameraPerspective>,listof(<TokenID>))\" >> beam.Map(\n",
    "            lambda tpl: (\n",
    "                tpl[0][0],  # <ASLConsultantID>\n",
    "                tpl[0][1],  # <TargetVideoFilename>\n",
    "                tpl[0][2],  # <UtteranceSequence>\n",
    "                tpl[0][3],  # <CameraPerspective>\n",
    "                [tcp_tpl[0] for tcp_tpl in tpl[1]] # listof(<TokenID>)\n",
    "            ) \n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we require this in order to make use of ib.show() (which provides visualization of the pcolls specified) or ib.collect() (which creates a pandas dataframe from a pcoll)\n",
    "    # but all pcolls we wish to visualize must be created prior to executing the following line\n",
    "ib.watch(locals())"
   ]
  },
  {
   "source": [
    "#### Show those with counts > 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n            <div id=\"progress_indicator_da2c819431d0f95f3bc2411f93416fe3\" class=\"spinner-border text-info\" role=\"status\">\n            </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n            $(\"#progress_indicator_da2c819431d0f95f3bc2411f93416fe3\").remove();\n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n            $(\"#progress_indicator_da2c819431d0f95f3bc2411f93416fe3\").remove();\n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_ctvusts_by_tcp__gt_1 = ib.collect(ctvusts_by_tcp__gt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           ASLConsultantID           TargetVideoFilename  \\\n",
       "TokenID CameraPerspective                                                  \n",
       "1       0                                1     ben_story_439_small_0.mov   \n",
       "        0                                1     ben_story_445_small_0.mov   \n",
       "        0                                1     ben_story_445_small_0.mov   \n",
       "        0                                1     ben_story_445_small_0.mov   \n",
       "        0                                2   lapd_story_1083_small_0.mov   \n",
       "...                                    ...                           ...   \n",
       "2370    0                                2  biker_buddy_1069_small_0.mov   \n",
       "        2                                2  biker_buddy_1069_small_2.mov   \n",
       "        2                                2  biker_buddy_1069_small_2.mov   \n",
       "2389    0                                2     football_1073_small_0.mov   \n",
       "        0                                7      DSP%2520Introduction.mov   \n",
       "\n",
       "                           UtteranceSequence  TokenSequence  \n",
       "TokenID CameraPerspective                                    \n",
       "1       0                                 31              0  \n",
       "        0                                 58              7  \n",
       "        0                                 59              0  \n",
       "        0                                 59              3  \n",
       "        0                                 20              1  \n",
       "...                                      ...            ...  \n",
       "2370    0                                 12              7  \n",
       "        2                                  0              9  \n",
       "        2                                 12              7  \n",
       "2389    0                                 48              3  \n",
       "        0                                  5              1  \n",
       "\n",
       "[29670 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">1</th>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_439_small_0.mov</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_445_small_0.mov</td>\n      <td>58</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_445_small_0.mov</td>\n      <td>59</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_445_small_0.mov</td>\n      <td>59</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>lapd_story_1083_small_0.mov</td>\n      <td>20</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">2370</th>\n      <th>0</th>\n      <td>2</td>\n      <td>biker_buddy_1069_small_0.mov</td>\n      <td>12</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>biker_buddy_1069_small_2.mov</td>\n      <td>0</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>biker_buddy_1069_small_2.mov</td>\n      <td>12</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">2389</th>\n      <th>0</th>\n      <td>2</td>\n      <td>football_1073_small_0.mov</td>\n      <td>48</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>DSP%2520Introduction.mov</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>29670 rows  4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df_ctvusts_by_tcp__gt_1.columns = ['TokenID', 'CameraPerspective', 'ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence']\n",
    "df_ctvusts_by_tcp__gt_1.set_index(['TokenID', 'CameraPerspective'], inplace=True)\n",
    "df_ctvusts_by_tcp__gt_1.sort_values(axis=0, by=['ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence'], ignore_index=False, inplace=True)\n",
    "df_ctvusts_by_tcp__gt_1.sort_index(inplace=True)\n",
    "df_ctvusts_by_tcp__gt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ctvusts_by_tcp__gt_1.loc[\n",
    "#     (\n",
    "#         [2369],         # TokenID\n",
    "#         [2]             # CameraPerspective\n",
    "#     ), \n",
    "#     :\n",
    "# ].sort_index(ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           count\n",
       "TokenID CameraPerspective       \n",
       "1403    0                    691\n",
       "594     0                    631\n",
       "1403    2                    575\n",
       "594     2                    562\n",
       "611     0                    406\n",
       "...                          ...\n",
       "653     3                      2\n",
       "655     0                      2\n",
       "658     2                      2\n",
       "669     3                      2\n",
       "2389    0                      2\n",
       "\n",
       "[3081 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>count</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1403</th>\n      <th>0</th>\n      <td>691</td>\n    </tr>\n    <tr>\n      <th>594</th>\n      <th>0</th>\n      <td>631</td>\n    </tr>\n    <tr>\n      <th>1403</th>\n      <th>2</th>\n      <td>575</td>\n    </tr>\n    <tr>\n      <th>594</th>\n      <th>2</th>\n      <td>562</td>\n    </tr>\n    <tr>\n      <th>611</th>\n      <th>0</th>\n      <td>406</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>653</th>\n      <th>3</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>655</th>\n      <th>0</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>658</th>\n      <th>2</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>669</th>\n      <th>3</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2389</th>\n      <th>0</th>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>3081 rows  1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "df_ctvusts_by_tcp__gt_1__count = df_ctvusts_by_tcp__gt_1.reset_index().groupby(['TokenID', 'CameraPerspective']).count()\n",
    "df_ctvusts_by_tcp__gt_1__count = df_ctvusts_by_tcp__gt_1__count[['ASLConsultantID']]\n",
    "df_ctvusts_by_tcp__gt_1__count.columns = ['count']\n",
    "df_ctvusts_by_tcp__gt_1__count.sort_values(axis=0, by=['count'], ascending=False, inplace=True)\n",
    "# df_ctvusts_by_tcp__gt_1__count.sort_index(inplace=True)\n",
    "df_ctvusts_by_tcp__gt_1__count"
   ]
  },
  {
   "source": [
    "#### Now show those with counts <= 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n            <div id=\"progress_indicator_d33045f5681b11f2f8e15b2138ed1e66\" class=\"spinner-border text-info\" role=\"status\">\n            </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n            $(\"#progress_indicator_d33045f5681b11f2f8e15b2138ed1e66\").remove();\n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n            $(\"#progress_indicator_d33045f5681b11f2f8e15b2138ed1e66\").remove();\n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_ctvusts_by_tcp__lte_1 = ib.collect(ctvusts_by_tcp__lte_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           ASLConsultantID         TargetVideoFilename  \\\n",
       "TokenID CameraPerspective                                                \n",
       "0       0                                1   ben_story_439_small_0.mov   \n",
       "        1                                1   ben_story_439_small_1.mov   \n",
       "        2                                1   ben_story_439_small_2.mov   \n",
       "        3                                1   ben_story_439_small_3.mov   \n",
       "2       0                                1   ben_story_439_small_0.mov   \n",
       "...                                    ...                         ...   \n",
       "2409    0                                5      DSP%2520Immigrants.mov   \n",
       "2410    0                                2  boston-la_1088_small_0.mov   \n",
       "        2                                2  boston-la_1088_small_2.mov   \n",
       "2411    0                                2  boston-la_1088_small_0.mov   \n",
       "        2                                2  boston-la_1088_small_2.mov   \n",
       "\n",
       "                           UtteranceSequence  TokenSequence  \n",
       "TokenID CameraPerspective                                    \n",
       "0       0                                  6              5  \n",
       "        1                                  6              5  \n",
       "        2                                  6              5  \n",
       "        3                                  6              5  \n",
       "2       0                                 47              1  \n",
       "...                                      ...            ...  \n",
       "2409    0                                  7              1  \n",
       "2410    0                                 22              3  \n",
       "        2                                 22              3  \n",
       "2411    0                                 71              8  \n",
       "        2                                 71              8  \n",
       "\n",
       "[3667 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">0</th>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_439_small_0.mov</td>\n      <td>6</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>ben_story_439_small_1.mov</td>\n      <td>6</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>ben_story_439_small_2.mov</td>\n      <td>6</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>ben_story_439_small_3.mov</td>\n      <td>6</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_439_small_0.mov</td>\n      <td>47</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2409</th>\n      <th>0</th>\n      <td>5</td>\n      <td>DSP%2520Immigrants.mov</td>\n      <td>7</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">2410</th>\n      <th>0</th>\n      <td>2</td>\n      <td>boston-la_1088_small_0.mov</td>\n      <td>22</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>boston-la_1088_small_2.mov</td>\n      <td>22</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">2411</th>\n      <th>0</th>\n      <td>2</td>\n      <td>boston-la_1088_small_0.mov</td>\n      <td>71</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>boston-la_1088_small_2.mov</td>\n      <td>71</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n<p>3667 rows  4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "df_ctvusts_by_tcp__lte_1.columns = ['TokenID', 'CameraPerspective', 'ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence']\n",
    "df_ctvusts_by_tcp__lte_1.set_index(['TokenID', 'CameraPerspective'], inplace=True)\n",
    "df_ctvusts_by_tcp__lte_1.sort_values(axis=0, by=['ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence'], ignore_index=False, inplace=True)\n",
    "df_ctvusts_by_tcp__lte_1.sort_index(inplace=True)\n",
    "df_ctvusts_by_tcp__lte_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           count\n",
       "TokenID CameraPerspective       \n",
       "0       0                      1\n",
       "1667    0                      1\n",
       "1668    2                      1\n",
       "1669    0                      1\n",
       "        2                      1\n",
       "...                          ...\n",
       "892     0                      1\n",
       "        2                      1\n",
       "893     0                      1\n",
       "894     0                      1\n",
       "2411    2                      1\n",
       "\n",
       "[3667 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>count</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1667</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1668</th>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">1669</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">892</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>893</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>894</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2411</th>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3667 rows  1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "df_ctvusts_by_tcp__lte_1__count = df_ctvusts_by_tcp__lte_1.reset_index().groupby(['TokenID', 'CameraPerspective']).count()\n",
    "df_ctvusts_by_tcp__lte_1__count = df_ctvusts_by_tcp__lte_1__count[['ASLConsultantID']]\n",
    "df_ctvusts_by_tcp__lte_1__count.columns = ['count']\n",
    "df_ctvusts_by_tcp__lte_1__count.sort_values(axis=0, by=['count'], ascending=False, inplace=True)\n",
    "# df_ctvusts_by_tcp__gt_1__count.sort_index(inplace=True)\n",
    "df_ctvusts_by_tcp__lte_1__count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ASLConsultantID_left, TargetVideoFilename_left, UtteranceSequence_left, TokenSequence_left, ASLConsultantID_right, TargetVideoFilename_right, UtteranceSequence_right, TokenSequence_right]\n",
       "Index: []"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID_left</th>\n      <th>TargetVideoFilename_left</th>\n      <th>UtteranceSequence_left</th>\n      <th>TokenSequence_left</th>\n      <th>ASLConsultantID_right</th>\n      <th>TargetVideoFilename_right</th>\n      <th>UtteranceSequence_right</th>\n      <th>TokenSequence_right</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "df_ctvusts_by_tcp__intersection = df_ctvusts_by_tcp__gt_1.join(df_ctvusts_by_tcp__lte_1, how='inner', lsuffix='_left', rsuffix='_right')\n",
    "df_ctvusts_by_tcp__intersection"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Now show train/validation split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n            <div id=\"progress_indicator_2efb94feea65cb3b71aaa8207a12cbb2\" class=\"spinner-border text-info\" role=\"status\">\n            </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n            $(\"#progress_indicator_2efb94feea65cb3b71aaa8207a12cbb2\").remove();\n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n            $(\"#progress_indicator_2efb94feea65cb3b71aaa8207a12cbb2\").remove();\n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_train__ctvusts_by_tcp__gt_1 = ib.collect(train__ctvusts_by_tcp__gt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           ASLConsultantID           TargetVideoFilename  \\\n",
       "TokenID CameraPerspective                                                  \n",
       "1       0                                1     ben_story_439_small_0.mov   \n",
       "        0                                1     ben_story_445_small_0.mov   \n",
       "        0                                1     ben_story_445_small_0.mov   \n",
       "        0                                2   lapd_story_1083_small_0.mov   \n",
       "        0                                2   lapd_story_1083_small_0.mov   \n",
       "...                                    ...                           ...   \n",
       "2368    0                                2    boston-la_1088_small_0.mov   \n",
       "        2                                2    boston-la_1088_small_2.mov   \n",
       "2370    0                                2  biker_buddy_1069_small_0.mov   \n",
       "        2                                2  biker_buddy_1069_small_2.mov   \n",
       "2389    0                                7      DSP%2520Introduction.mov   \n",
       "\n",
       "                           UtteranceSequence  TokenSequence  \n",
       "TokenID CameraPerspective                                    \n",
       "1       0                                 31              0  \n",
       "        0                                 58              7  \n",
       "        0                                 59              0  \n",
       "        0                                 20              1  \n",
       "        0                                 21              1  \n",
       "...                                      ...            ...  \n",
       "2368    0                                 25             11  \n",
       "        2                                 25             11  \n",
       "2370    0                                  0              9  \n",
       "        2                                 12              7  \n",
       "2389    0                                  5              1  \n",
       "\n",
       "[25290 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">1</th>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_439_small_0.mov</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_445_small_0.mov</td>\n      <td>58</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_445_small_0.mov</td>\n      <td>59</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>lapd_story_1083_small_0.mov</td>\n      <td>20</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>lapd_story_1083_small_0.mov</td>\n      <td>21</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">2368</th>\n      <th>0</th>\n      <td>2</td>\n      <td>boston-la_1088_small_0.mov</td>\n      <td>25</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>boston-la_1088_small_2.mov</td>\n      <td>25</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">2370</th>\n      <th>0</th>\n      <td>2</td>\n      <td>biker_buddy_1069_small_0.mov</td>\n      <td>0</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>biker_buddy_1069_small_2.mov</td>\n      <td>12</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2389</th>\n      <th>0</th>\n      <td>7</td>\n      <td>DSP%2520Introduction.mov</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>25290 rows  4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "df_train__ctvusts_by_tcp__gt_1.columns = ['TokenID', 'CameraPerspective', 'ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence']\n",
    "df_train__ctvusts_by_tcp__gt_1.set_index(['TokenID', 'CameraPerspective'], inplace=True)\n",
    "df_train__ctvusts_by_tcp__gt_1.sort_values(axis=0, by=['ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence'], ignore_index=False, inplace=True)\n",
    "df_train__ctvusts_by_tcp__gt_1.sort_index(inplace=True)\n",
    "df_train__ctvusts_by_tcp__gt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n            <div id=\"progress_indicator_56bf2a9f05039d9fb6bfe523dde1b9ec\" class=\"spinner-border text-info\" role=\"status\">\n            </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n            $(\"#progress_indicator_56bf2a9f05039d9fb6bfe523dde1b9ec\").remove();\n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n            $(\"#progress_indicator_56bf2a9f05039d9fb6bfe523dde1b9ec\").remove();\n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_val__ctvusts_by_tcp__gt_1 = ib.collect(val__ctvusts_by_tcp__gt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           ASLConsultantID           TargetVideoFilename  \\\n",
       "TokenID CameraPerspective                                                  \n",
       "1       0                                1     ben_story_445_small_0.mov   \n",
       "        1                                1     ben_story_445_small_1.mov   \n",
       "        2                                2   lapd_story_1083_small_2.mov   \n",
       "        3                                1     ben_story_445_small_3.mov   \n",
       "3       0                                1     ben_story_443_small_0.mov   \n",
       "...                                    ...                           ...   \n",
       "2368    0                                2    boston-la_1088_small_0.mov   \n",
       "        2                                2    boston-la_1088_small_2.mov   \n",
       "2370    0                                2  biker_buddy_1069_small_0.mov   \n",
       "        2                                2  biker_buddy_1069_small_2.mov   \n",
       "2389    0                                2     football_1073_small_0.mov   \n",
       "\n",
       "                           UtteranceSequence  TokenSequence  \n",
       "TokenID CameraPerspective                                    \n",
       "1       0                                 59              3  \n",
       "        1                                 55              4  \n",
       "        2                                 20              1  \n",
       "        3                                 59              3  \n",
       "3       0                                 48              4  \n",
       "...                                      ...            ...  \n",
       "2368    0                                 25             13  \n",
       "        2                                 25             13  \n",
       "2370    0                                 12              7  \n",
       "        2                                  0              9  \n",
       "2389    0                                 48              3  \n",
       "\n",
       "[4380 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">1</th>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_445_small_0.mov</td>\n      <td>59</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>ben_story_445_small_1.mov</td>\n      <td>55</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>lapd_story_1083_small_2.mov</td>\n      <td>20</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>ben_story_445_small_3.mov</td>\n      <td>59</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_443_small_0.mov</td>\n      <td>48</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">2368</th>\n      <th>0</th>\n      <td>2</td>\n      <td>boston-la_1088_small_0.mov</td>\n      <td>25</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>boston-la_1088_small_2.mov</td>\n      <td>25</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">2370</th>\n      <th>0</th>\n      <td>2</td>\n      <td>biker_buddy_1069_small_0.mov</td>\n      <td>12</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>biker_buddy_1069_small_2.mov</td>\n      <td>0</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2389</th>\n      <th>0</th>\n      <td>2</td>\n      <td>football_1073_small_0.mov</td>\n      <td>48</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>4380 rows  4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "df_val__ctvusts_by_tcp__gt_1.columns = ['TokenID', 'CameraPerspective', 'ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence']\n",
    "df_val__ctvusts_by_tcp__gt_1.set_index(['TokenID', 'CameraPerspective'], inplace=True)\n",
    "df_val__ctvusts_by_tcp__gt_1.sort_values(axis=0, by=['ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence'], ignore_index=False, inplace=True)\n",
    "df_val__ctvusts_by_tcp__gt_1.sort_index(inplace=True)\n",
    "df_val__ctvusts_by_tcp__gt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           ASLConsultantID       TargetVideoFilename  \\\n",
       "TokenID CameraPerspective                                              \n",
       "2389    0                                7  DSP%2520Introduction.mov   \n",
       "\n",
       "                           UtteranceSequence  TokenSequence  \n",
       "TokenID CameraPerspective                                    \n",
       "2389    0                                  5              1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2389</th>\n      <th>0</th>\n      <td>7</td>\n      <td>DSP%2520Introduction.mov</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "df_train__ctvusts_by_tcp__gt_1.loc[\n",
    "    (\n",
    "        [2389],         # TokenID\n",
    "        [0]             # CameraPerspective\n",
    "    ), \n",
    "    :\n",
    "].sort_index(ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           ASLConsultantID        TargetVideoFilename  \\\n",
       "TokenID CameraPerspective                                               \n",
       "2389    0                                2  football_1073_small_0.mov   \n",
       "\n",
       "                           UtteranceSequence  TokenSequence  \n",
       "TokenID CameraPerspective                                    \n",
       "2389    0                                 48              3  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2389</th>\n      <th>0</th>\n      <td>2</td>\n      <td>football_1073_small_0.mov</td>\n      <td>48</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "df_val__ctvusts_by_tcp__gt_1.loc[\n",
    "    (\n",
    "        [2389],         # TokenID\n",
    "        [0]             # CameraPerspective\n",
    "    ), \n",
    "    :\n",
    "].sort_index(ascending=[True, True])"
   ]
  },
  {
   "source": [
    "<p><br>\n",
    "\n",
    "#### View final training/validation sets (with associated frame sequences)\n",
    "\n",
    "<p><br>\n",
    "\n",
    "##### Training (sub) set (that has at least one corresponding token/camera perspective in the validation set)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n            <div id=\"progress_indicator_80bb8151c8295b1d0de307b1d9e6b0e6\" class=\"spinner-border text-info\" role=\"status\">\n            </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n            $(\"#progress_indicator_80bb8151c8295b1d0de307b1d9e6b0e6\").remove();\n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n            $(\"#progress_indicator_80bb8151c8295b1d0de307b1d9e6b0e6\").remove();\n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_train_dctvustsfs__gt__1 = ib.collect(train_dctvustsfs__gt__1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           ASLConsultantID        TargetVideoFilename  \\\n",
       "TokenID CameraPerspective                                               \n",
       "1       0                                1  ben_story_439_small_0.mov   \n",
       "        0                                1  ben_story_439_small_0.mov   \n",
       "        0                                1  ben_story_439_small_0.mov   \n",
       "        0                                1  ben_story_439_small_0.mov   \n",
       "        0                                1  ben_story_439_small_0.mov   \n",
       "...                                    ...                        ...   \n",
       "2389    0                                7   DSP%2520Introduction.mov   \n",
       "        0                                7   DSP%2520Introduction.mov   \n",
       "        0                                7   DSP%2520Introduction.mov   \n",
       "        0                                7   DSP%2520Introduction.mov   \n",
       "        0                                7   DSP%2520Introduction.mov   \n",
       "\n",
       "                           UtteranceSequence  TokenSequence  FrameSequence  \n",
       "TokenID CameraPerspective                                                   \n",
       "1       0                                 31              0           2282  \n",
       "        0                                 31              0           2283  \n",
       "        0                                 31              0           2284  \n",
       "        0                                 31              0           2285  \n",
       "        0                                 31              0           2286  \n",
       "...                                      ...            ...            ...  \n",
       "2389    0                                  5              1            434  \n",
       "        0                                  5              1            435  \n",
       "        0                                  5              1            436  \n",
       "        0                                  5              1            437  \n",
       "        0                                  5              1            438  \n",
       "\n",
       "[194179 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n      <th>FrameSequence</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">1</th>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_439_small_0.mov</td>\n      <td>31</td>\n      <td>0</td>\n      <td>2282</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_439_small_0.mov</td>\n      <td>31</td>\n      <td>0</td>\n      <td>2283</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_439_small_0.mov</td>\n      <td>31</td>\n      <td>0</td>\n      <td>2284</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_439_small_0.mov</td>\n      <td>31</td>\n      <td>0</td>\n      <td>2285</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_439_small_0.mov</td>\n      <td>31</td>\n      <td>0</td>\n      <td>2286</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2389</th>\n      <th>0</th>\n      <td>7</td>\n      <td>DSP%2520Introduction.mov</td>\n      <td>5</td>\n      <td>1</td>\n      <td>434</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>DSP%2520Introduction.mov</td>\n      <td>5</td>\n      <td>1</td>\n      <td>435</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>DSP%2520Introduction.mov</td>\n      <td>5</td>\n      <td>1</td>\n      <td>436</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>DSP%2520Introduction.mov</td>\n      <td>5</td>\n      <td>1</td>\n      <td>437</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>DSP%2520Introduction.mov</td>\n      <td>5</td>\n      <td>1</td>\n      <td>438</td>\n    </tr>\n  </tbody>\n</table>\n<p>194179 rows  5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "df_train_dctvustsfs__gt__1.columns = ['TokenID', 'CameraPerspective', 'ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence', 'FrameSequence']\n",
    "df_train_dctvustsfs__gt__1.set_index(['TokenID', 'CameraPerspective'], inplace=True)\n",
    "df_train_dctvustsfs__gt__1.sort_values(axis=0, by=['ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence', 'FrameSequence'], ignore_index=False, inplace=True)\n",
    "df_train_dctvustsfs__gt__1.sort_index(inplace=True)\n",
    "df_train_dctvustsfs__gt__1"
   ]
  },
  {
   "source": [
    "<p><br>\n",
    "\n",
    "##### Validation set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n            <div id=\"progress_indicator_15da4f81ab0a0d98a890595093985500\" class=\"spinner-border text-info\" role=\"status\">\n            </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n            $(\"#progress_indicator_15da4f81ab0a0d98a890595093985500\").remove();\n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n            $(\"#progress_indicator_15da4f81ab0a0d98a890595093985500\").remove();\n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_val_dctvustsfs__gt__1 = ib.collect(val_dctvustsfs__gt__1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           ASLConsultantID        TargetVideoFilename  \\\n",
       "TokenID CameraPerspective                                               \n",
       "1       0                                1  ben_story_445_small_0.mov   \n",
       "        0                                1  ben_story_445_small_0.mov   \n",
       "        0                                1  ben_story_445_small_0.mov   \n",
       "        0                                1  ben_story_445_small_0.mov   \n",
       "        0                                1  ben_story_445_small_0.mov   \n",
       "...                                    ...                        ...   \n",
       "2389    0                                2  football_1073_small_0.mov   \n",
       "        0                                2  football_1073_small_0.mov   \n",
       "        0                                2  football_1073_small_0.mov   \n",
       "        0                                2  football_1073_small_0.mov   \n",
       "        0                                2  football_1073_small_0.mov   \n",
       "\n",
       "                           UtteranceSequence  TokenSequence  FrameSequence  \n",
       "TokenID CameraPerspective                                                   \n",
       "1       0                                 59              3           1641  \n",
       "        0                                 59              3           1642  \n",
       "        0                                 59              3           1643  \n",
       "        0                                 59              3           1644  \n",
       "        0                                 59              3           1645  \n",
       "...                                      ...            ...            ...  \n",
       "2389    0                                 48              3           5019  \n",
       "        0                                 48              3           5020  \n",
       "        0                                 48              3           5021  \n",
       "        0                                 48              3           5022  \n",
       "        0                                 48              3           5023  \n",
       "\n",
       "[40436 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n      <th>FrameSequence</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">1</th>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_445_small_0.mov</td>\n      <td>59</td>\n      <td>3</td>\n      <td>1641</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_445_small_0.mov</td>\n      <td>59</td>\n      <td>3</td>\n      <td>1642</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_445_small_0.mov</td>\n      <td>59</td>\n      <td>3</td>\n      <td>1643</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_445_small_0.mov</td>\n      <td>59</td>\n      <td>3</td>\n      <td>1644</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_445_small_0.mov</td>\n      <td>59</td>\n      <td>3</td>\n      <td>1645</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2389</th>\n      <th>0</th>\n      <td>2</td>\n      <td>football_1073_small_0.mov</td>\n      <td>48</td>\n      <td>3</td>\n      <td>5019</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>football_1073_small_0.mov</td>\n      <td>48</td>\n      <td>3</td>\n      <td>5020</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>football_1073_small_0.mov</td>\n      <td>48</td>\n      <td>3</td>\n      <td>5021</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>football_1073_small_0.mov</td>\n      <td>48</td>\n      <td>3</td>\n      <td>5022</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>football_1073_small_0.mov</td>\n      <td>48</td>\n      <td>3</td>\n      <td>5023</td>\n    </tr>\n  </tbody>\n</table>\n<p>40436 rows  5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "df_val_dctvustsfs__gt__1.columns = ['TokenID', 'CameraPerspective', 'ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence', 'FrameSequence']\n",
    "df_val_dctvustsfs__gt__1.set_index(['TokenID', 'CameraPerspective'], inplace=True)\n",
    "df_val_dctvustsfs__gt__1.sort_values(axis=0, by=['ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence', 'FrameSequence'], ignore_index=False, inplace=True)\n",
    "df_val_dctvustsfs__gt__1.sort_index(inplace=True)\n",
    "df_val_dctvustsfs__gt__1"
   ]
  },
  {
   "source": [
    "##### The complete training set (union of training subset - token/camera perspectives with corresponding validation set tuples - with training subset with no corresponding validation set tuples)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n            <div id=\"progress_indicator_78d9a918a666120e5f9b35f7a6056238\" class=\"spinner-border text-info\" role=\"status\">\n            </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n            $(\"#progress_indicator_78d9a918a666120e5f9b35f7a6056238\").remove();\n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n            $(\"#progress_indicator_78d9a918a666120e5f9b35f7a6056238\").remove();\n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_train_dctvustsfs__all = ib.collect(train_dctvustsfs__all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           ASLConsultantID         TargetVideoFilename  \\\n",
       "TokenID CameraPerspective                                                \n",
       "0       0                                1   ben_story_439_small_0.mov   \n",
       "        0                                1   ben_story_439_small_0.mov   \n",
       "        0                                1   ben_story_439_small_0.mov   \n",
       "        0                                1   ben_story_439_small_0.mov   \n",
       "        0                                1   ben_story_439_small_0.mov   \n",
       "...                                    ...                         ...   \n",
       "2411    0                                2  boston-la_1088_small_0.mov   \n",
       "        2                                2  boston-la_1088_small_2.mov   \n",
       "        2                                2  boston-la_1088_small_2.mov   \n",
       "        2                                2  boston-la_1088_small_2.mov   \n",
       "        2                                2  boston-la_1088_small_2.mov   \n",
       "\n",
       "                           UtteranceSequence  TokenSequence  FrameSequence  \n",
       "TokenID CameraPerspective                                                   \n",
       "0       0                                  6              5            559  \n",
       "        0                                  6              5            560  \n",
       "        0                                  6              5            561  \n",
       "        0                                  6              5            562  \n",
       "        0                                  6              5            563  \n",
       "...                                      ...            ...            ...  \n",
       "2411    0                                 71              8          11514  \n",
       "        2                                 71              8          11511  \n",
       "        2                                 71              8          11512  \n",
       "        2                                 71              8          11513  \n",
       "        2                                 71              8          11514  \n",
       "\n",
       "[247935 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n      <th>FrameSequence</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_439_small_0.mov</td>\n      <td>6</td>\n      <td>5</td>\n      <td>559</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_439_small_0.mov</td>\n      <td>6</td>\n      <td>5</td>\n      <td>560</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_439_small_0.mov</td>\n      <td>6</td>\n      <td>5</td>\n      <td>561</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_439_small_0.mov</td>\n      <td>6</td>\n      <td>5</td>\n      <td>562</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_439_small_0.mov</td>\n      <td>6</td>\n      <td>5</td>\n      <td>563</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2411</th>\n      <th>0</th>\n      <td>2</td>\n      <td>boston-la_1088_small_0.mov</td>\n      <td>71</td>\n      <td>8</td>\n      <td>11514</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>boston-la_1088_small_2.mov</td>\n      <td>71</td>\n      <td>8</td>\n      <td>11511</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>boston-la_1088_small_2.mov</td>\n      <td>71</td>\n      <td>8</td>\n      <td>11512</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>boston-la_1088_small_2.mov</td>\n      <td>71</td>\n      <td>8</td>\n      <td>11513</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>boston-la_1088_small_2.mov</td>\n      <td>71</td>\n      <td>8</td>\n      <td>11514</td>\n    </tr>\n  </tbody>\n</table>\n<p>247935 rows  5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "df_train_dctvustsfs__all.columns = ['TokenID', 'CameraPerspective', 'ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence', 'FrameSequence']\n",
    "df_train_dctvustsfs__all.set_index(['TokenID', 'CameraPerspective'], inplace=True)\n",
    "df_train_dctvustsfs__all.sort_values(axis=0, by=['ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence', 'FrameSequence'], ignore_index=False, inplace=True)\n",
    "df_train_dctvustsfs__all.sort_index(inplace=True)\n",
    "df_train_dctvustsfs__all"
   ]
  },
  {
   "source": [
    "<p><br>\n",
    "\n",
    "##### Show (complete) utterances that can be represented by token-cameraperspective tuples from the validation set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n            <div id=\"progress_indicator_5d16a76295ba0ee6e1628063c1cd1879\" class=\"spinner-border text-info\" role=\"status\">\n            </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n            $(\"#progress_indicator_5d16a76295ba0ee6e1628063c1cd1879\").remove();\n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n            $(\"#progress_indicator_5d16a76295ba0ee6e1628063c1cd1879\").remove();\n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_complete_utterances__with__val_tcp__gt__1 = ib.collect(complete_utterances__with__val_tcp__gt__1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                                                            TokenIDSequence\n",
       "ASLConsultantID TargetVideoFilename      UtteranceSequence CameraPerspective                                               \n",
       "0               DSP%2520DeadDog.mov      5                 0                     [499, 655, 1865, 188, 359, 731, 1014, 655]\n",
       "                                         9                 0                                          [537, 414, 1134, 413]\n",
       "                                         12                0                                        [433, 1403, 1428, 1403]\n",
       "                                         13                0                                                     [926, 495]\n",
       "                                         23                0                                    [1459, 461, 1658, 73, 1922]\n",
       "...                                                                                                                     ...\n",
       "6               d9_1165_small_2.mov      36                2                                         [611, 2136, 1649, 325]\n",
       "                d9_1165_small_3.mov      36                3                                         [611, 2136, 1649, 325]\n",
       "7               DSP%2520Introduction.mov 1                 0                                              [1679, 710, 2239]\n",
       "                                         7                 0                  [921, 1025, 1679, 924, 2118, 943, 2239, 1403]\n",
       "                                         11                0                                                     [315, 388]\n",
       "\n",
       "[3000 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>TokenIDSequence</th>\n    </tr>\n    <tr>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>UtteranceSequence</th>\n      <th>CameraPerspective</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th rowspan=\"5\" valign=\"top\">DSP%2520DeadDog.mov</th>\n      <th>5</th>\n      <th>0</th>\n      <td>[499, 655, 1865, 188, 359, 731, 1014, 655]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <th>0</th>\n      <td>[537, 414, 1134, 413]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <th>0</th>\n      <td>[433, 1403, 1428, 1403]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <th>0</th>\n      <td>[926, 495]</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <th>0</th>\n      <td>[1459, 461, 1658, 73, 1922]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">6</th>\n      <th>d9_1165_small_2.mov</th>\n      <th>36</th>\n      <th>2</th>\n      <td>[611, 2136, 1649, 325]</td>\n    </tr>\n    <tr>\n      <th>d9_1165_small_3.mov</th>\n      <th>36</th>\n      <th>3</th>\n      <td>[611, 2136, 1649, 325]</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">7</th>\n      <th rowspan=\"3\" valign=\"top\">DSP%2520Introduction.mov</th>\n      <th>1</th>\n      <th>0</th>\n      <td>[1679, 710, 2239]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <th>0</th>\n      <td>[921, 1025, 1679, 924, 2118, 943, 2239, 1403]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <th>0</th>\n      <td>[315, 388]</td>\n    </tr>\n  </tbody>\n</table>\n<p>3000 rows  1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "df_complete_utterances__with__val_tcp__gt__1.columns = ['ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'CameraPerspective', 'TokenIDSequence']\n",
    "df_complete_utterances__with__val_tcp__gt__1.set_index(['ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'CameraPerspective'], inplace=True)\n",
    "df_complete_utterances__with__val_tcp__gt__1.sort_index(inplace=True)\n",
    "df_complete_utterances__with__val_tcp__gt__1"
   ]
  }
 ]
}