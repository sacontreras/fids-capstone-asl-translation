{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Anaconda: learn-env",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "import apache_beam.transforms.sql\n",
    "\n",
    "import beam__common\n",
    "import fidscs_globals\n",
    "import random\n",
    "\n",
    "from importlib import import_module\n",
    "data_extractor = import_module('data-extractor', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "use_beam: True\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CollectiveCommunication.AUTO\n",
      "Number of devices available for parallel processing: 1\n",
      "PipelineOptions:\n",
      "{'runner': 'DirectRunner', 'streaming': False, 'beam_services': {}, 'type_check_strictness': 'DEFAULT_TO_ANY', 'type_check_additional': '', 'pipeline_type_check': True, 'runtime_type_check': False, 'performance_runtime_type_check': False, 'direct_runner_use_stacked_bundle': True, 'direct_runner_bundle_repeat': 0, 'direct_num_workers': 0, 'direct_running_mode': 'multi_threading', 'dataflow_endpoint': 'https://dataflow.googleapis.com', 'project': 'my-project', 'job_name': None, 'staging_location': None, 'temp_location': None, 'region': None, 'service_account_email': None, 'no_auth': False, 'template_location': None, 'labels': None, 'update': False, 'transform_name_mapping': None, 'enable_streaming_engine': False, 'dataflow_kms_key': None, 'flexrs_goal': None, 'hdfs_host': None, 'hdfs_port': None, 'hdfs_user': None, 'hdfs_full_urls': False, 'num_workers': None, 'max_num_workers': None, 'autoscaling_algorithm': None, 'machine_type': None, 'disk_size_gb': None, 'disk_type': None, 'worker_region': None, 'worker_zone': None, 'zone': None, 'network': None, 'subnetwork': None, 'worker_harness_container_image': None, 'sdk_harness_container_image_overrides': None, 'use_public_ips': None, 'min_cpu_platform': None, 'dataflow_worker_jar': None, 'dataflow_job_file': None, 'experiments': None, 'number_of_worker_harness_threads': None, 'profile_cpu': False, 'profile_memory': False, 'profile_location': None, 'profile_sample_rate': 1.0, 'requirements_file': None, 'requirements_cache': None, 'setup_file': None, 'beam_plugins': None, 'save_main_session': False, 'sdk_location': 'default', 'extra_packages': None, 'prebuild_sdk_container_engine': None, 'prebuild_sdk_container_base_image': None, 'docker_registry_push_url': None, 'job_endpoint': None, 'artifact_endpoint': None, 'job_server_timeout': 60, 'environment_type': None, 'environment_config': None, 'environment_options': None, 'sdk_worker_parallelism': 1, 'environment_cache_millis': 0, 'output_executable_path': None, 'artifacts_dir': None, 'job_port': 0, 'artifact_port': 0, 'expansion_port': 0, 'flink_master': '[auto]', 'flink_version': '1.10', 'flink_job_server_jar': None, 'flink_submit_uber_jar': False, 'spark_master_url': 'local[4]', 'spark_job_server_jar': None, 'spark_submit_uber_jar': False, 'spark_rest_url': None, 'on_success_matcher': None, 'dry_run': False, 'wait_until_finish_duration': None, 'pubsubRootUrl': None, 's3_access_key_id': None, 's3_secret_access_key': None, 's3_session_token': None, 's3_endpoint_url': None, 's3_region_name': None, 's3_api_version': None, 's3_verify': None, 's3_disable_ssl': False}\n",
      "\n",
      "Found dataset /tmp/fids-capstone-data/data/consultant-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-targetvideo-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-utterance-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-utterance-targetvideo-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-utterance-token-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/ncslgr-corpus-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-targetvideo-utterance-token-frame-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/vocabulary-index.csv\n",
      "Beam PL: ALL DONE!\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/tmp/fids-capstone-data/data\"\n",
    "\n",
    "data_extractor.run(max_target_videos=-1, data_dir=data_dir, use_beam=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PipelineOptions:\n{'runner': 'InteractiveRunner', 'streaming': False, 'beam_services': {}, 'type_check_strictness': 'DEFAULT_TO_ANY', 'type_check_additional': '', 'pipeline_type_check': True, 'runtime_type_check': False, 'performance_runtime_type_check': False, 'direct_runner_use_stacked_bundle': True, 'direct_runner_bundle_repeat': 0, 'direct_num_workers': 0, 'direct_running_mode': 'multi_threading', 'dataflow_endpoint': 'https://dataflow.googleapis.com', 'project': 'my-project', 'job_name': None, 'staging_location': None, 'temp_location': None, 'region': None, 'service_account_email': None, 'no_auth': False, 'template_location': None, 'labels': None, 'update': False, 'transform_name_mapping': None, 'enable_streaming_engine': False, 'dataflow_kms_key': None, 'flexrs_goal': None, 'hdfs_host': None, 'hdfs_port': None, 'hdfs_user': None, 'hdfs_full_urls': False, 'num_workers': None, 'max_num_workers': None, 'autoscaling_algorithm': None, 'machine_type': None, 'disk_size_gb': None, 'disk_type': None, 'worker_region': None, 'worker_zone': None, 'zone': None, 'network': None, 'subnetwork': None, 'worker_harness_container_image': None, 'sdk_harness_container_image_overrides': None, 'use_public_ips': None, 'min_cpu_platform': None, 'dataflow_worker_jar': None, 'dataflow_job_file': None, 'experiments': None, 'number_of_worker_harness_threads': None, 'profile_cpu': False, 'profile_memory': False, 'profile_location': None, 'profile_sample_rate': 1.0, 'requirements_file': None, 'requirements_cache': None, 'setup_file': None, 'beam_plugins': None, 'save_main_session': False, 'sdk_location': 'default', 'extra_packages': None, 'prebuild_sdk_container_engine': None, 'prebuild_sdk_container_base_image': None, 'docker_registry_push_url': None, 'job_endpoint': None, 'artifact_endpoint': None, 'job_server_timeout': 60, 'environment_type': None, 'environment_config': None, 'environment_options': None, 'sdk_worker_parallelism': 1, 'environment_cache_millis': 0, 'output_executable_path': None, 'artifacts_dir': None, 'job_port': 0, 'artifact_port': 0, 'expansion_port': 0, 'flink_master': '[auto]', 'flink_version': '1.10', 'flink_job_server_jar': None, 'flink_submit_uber_jar': False, 'spark_master_url': 'local[4]', 'spark_job_server_jar': None, 'spark_submit_uber_jar': False, 'spark_rest_url': None, 'on_success_matcher': None, 'dry_run': False, 'wait_until_finish_duration': None, 'pubsubRootUrl': None, 's3_access_key_id': None, 's3_secret_access_key': None, 's3_session_token': None, 's3_endpoint_url': None, 's3_region_name': None, 's3_api_version': None, 's3_verify': None, 's3_disable_ssl': False}\n\n"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "    'project': 'my-project', # change\n",
    "    # 'runner': 'DirectRunner',\n",
    "    'runner': 'InteractiveRunner',\n",
    "    'direct_num_workers': 0, # 0 is use all available cores\n",
    "    'direct_running_mode': 'multi_threading', # ['in_memory', 'multi_threading', 'multi_processing'] # 'multi_processing' doesn't seem to work for DirectRunner?\n",
    "    'streaming': False # set to True if data source is unbounded (e.g. GCP PubSub)\n",
    "}\n",
    "pipeline_options = PipelineOptions(flags=[], **options) # easier to pass in options from command-line this way\n",
    "print(f\"PipelineOptions:\\n{pipeline_options.get_all_options()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidscs_globals.DATA_ROOT_DIR = data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "can_proceed = True\n",
    "\n",
    "if not tf.io.gfile.exists(fidscs_globals.DATA_ROOT_DIR) or len(tf.io.gfile.listdir(fidscs_globals.DATA_ROOT_DIR))==0:\n",
    "    print(f\"{fidscs_globals.VALIDATION_FATAL_ERROR_TEXT} data directory does not exist or is empty!\")\n",
    "    can_proceed = False\n",
    "else:\n",
    "    fidscs_globals.VIDEO_DIR = os.path.join(fidscs_globals.DATA_ROOT_DIR, 'videos')\n",
    "    fidscs_globals.STICHED_VIDEO_FRAMES_DIR = os.path.join(fidscs_globals.DATA_ROOT_DIR, 'stitched_video_frames')\n",
    "    fidscs_globals.CORPUS_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.CORPUS_DS_FNAME)\n",
    "    fidscs_globals.DOCUMENT_ASL_CONSULTANT_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.DOCUMENT_ASL_CONSULTANT_DS_FNAME)\n",
    "    fidscs_globals.ASL_CONSULTANT_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.ASL_CONSULTANT_DS_FNAME)\n",
    "    fidscs_globals.VIDEO_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.VIDEO_DS_FNAME)\n",
    "    fidscs_globals.VIDEO_SEGMENT_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.VIDEO_SEGMENT_DS_FNAME)\n",
    "    fidscs_globals.VIDEO_FRAME_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.VIDEO_FRAME_DS_FNAME)\n",
    "    fidscs_globals.UTTERANCE_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.UTTERANCE_DS_FNAME)\n",
    "    fidscs_globals.UTTERANCE_VIDEO_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.UTTERANCE_VIDEO_DS_FNAME)\n",
    "    fidscs_globals.UTTERANCE_TOKEN_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.UTTERANCE_TOKEN_DS_FNAME)\n",
    "    fidscs_globals.UTTERANCE_TOKEN_FRAME_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.UTTERANCE_TOKEN_FRAME_DS_FNAME)\n",
    "    fidscs_globals.VOCABULARY_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.VOCABULARY_DS_FNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "pl = beam.Pipeline(options=pipeline_options)\n",
    "\n",
    "full_target_vid_index_schemad_pcoll = beam__common.pl__1__read_target_vid_index_csv(pl)\n",
    "corpus_index_schemad_pcoll = beam__common.pl__1__read_corpus_index_csv(pl) # XML is base-64 encode but we no longer need it (to decode it) since it is only used to create the datasets\n",
    "# corpus_index_decoded_XML_pcoll = pl__2__decode_XML(corpus_index_schemad_pcoll) # see above\n",
    "\n",
    "asl_consultant_index_schemad_pcoll = beam__common.pl__1__read_asl_consultant_index_csv(pl)\n",
    "document_asl_consultant_utterance_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_utterance_index_csv(pl)\n",
    "document_asl_consultant_target_video_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_target_video_index_csv(pl)\n",
    "document_asl_consultant_utterance_video_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_utterance_video_index_csv(pl)\n",
    "document_target_video_segment_index_schemad_pcoll = beam__common.pl__1__read_document_target_video_segment_index_csv(pl)\n",
    "vocabulary_index_schemad_pcoll = beam__common.pl__1__read_vocabulary_index_csv(pl)\n",
    "document_asl_consultant_utterance_token_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_utterance_token_index_csv(pl)\n",
    "document_asl_consultant_target_video_frame_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_target_video_frame_index_csv(pl)\n",
    "document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_target_video_utterance_token_frame_index_csv(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll is the main table we use for training.\n",
    "#     This will ultimately provide which frame sequences correspond to individual tokens.\n",
    "\n",
    "# But our first measure is to build train and validation sets (for tokens).\n",
    "#   In order to split up train vs validation sets, we need to compare \"apples to apples\".\n",
    "#   That is, in order for a token (TokenID) to be considered a candidate for the split,\n",
    "#   we require at least two of the same (TokenID, CameraPerspective) wherein the ASL\n",
    "#   consultant for each differs.  We would prefer more than two of these tuples, each\n",
    "#   having unique ASL consultants in the set of occurrences, with the majority of said\n",
    "#   tuples being assigned to the training set and the remainder (at least one) being\n",
    "#   assigned to the validation set.  We would like to achieve a 90/10 split, ideally,\n",
    "#   but we will take what we get.\n",
    "\n",
    "# document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll:\n",
    "    # beam.Row(\n",
    "    #   DocumentID=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[0]]),\n",
    "    #   ASLConsultantID=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[1]]),\n",
    "    #   CameraPerspective=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[2]]),\n",
    "    #   TargetVideoFilename=str(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[3]]),\n",
    "    #   UtteranceSequence=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[4]]),\n",
    "    #   TokenSequence=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[5]]),\n",
    "    #   FrameSequence=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[6]]),\n",
    "    #   TokenID=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[7]])\n",
    "    # )\n",
    "\n",
    "# We will transform this into tuples of the form:\n",
    "    # [\n",
    "    #     'TokenID', \n",
    "    #     'CameraPerspective', \n",
    "    #     'DocumentID', \n",
    "    #     'ASLConsultantID', \n",
    "    #     'TargetVideoFilename', \n",
    "    #     'UtteranceSequence', \n",
    "    #     'TokenSequence',\n",
    "\n",
    "    #     'FrameSequence'\n",
    "    # ]\n",
    "\n",
    "dctvustsfs = (\n",
    "    document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll\n",
    "    | \"Beam PL: extract (TokenID,CameraPerspective,ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence,FrameSequence) from dctvustsfs schemad pcoll\" >> beam.Map(\n",
    "            lambda dctvustsfs_row: (\n",
    "                dctvustsfs_row.TokenID,\n",
    "                dctvustsfs_row.CameraPerspective,\n",
    "                dctvustsfs_row.ASLConsultantID,\n",
    "                dctvustsfs_row.TargetVideoFilename,\n",
    "                dctvustsfs_row.UtteranceSequence,\n",
    "                dctvustsfs_row.TokenSequence,\n",
    "                dctvustsfs_row.FrameSequence\n",
    "            )\n",
    "        )\n",
    ")\n",
    "\n",
    "# for train-validation split, we want to key/group by (TokenID, CameraPerspective) with lists of unique (ASLConsultantID, TargetVideoFilename, UtteranceSequence, TokenSequence) > 1\n",
    "ctvusts_by_tcp = (\n",
    "    dctvustsfs\n",
    "    | \"Beam PL: extract ((TokenID,CameraPerspective), (ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence)) from dctvustsfs\" >> beam.Map(\n",
    "            lambda dctvustsfs_row_tpl: (\n",
    "                (\n",
    "                    dctvustsfs_row_tpl[0],\n",
    "                    dctvustsfs_row_tpl[1]\n",
    "                ),\n",
    "                (\n",
    "                    dctvustsfs_row_tpl[2],\n",
    "                    dctvustsfs_row_tpl[3],\n",
    "                    dctvustsfs_row_tpl[4],\n",
    "                    dctvustsfs_row_tpl[5]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    | \"Beam PL: select distinct ((TokenID,CameraPerspective), (ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence)) from ctvusts_by_tcp\" >> beam.Distinct()\n",
    "    | \"Beam PL: group (ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence) by key (TokenID,CameraPerspective)\" >> beam.GroupByKey() \n",
    "    # the above produces tuples of the form:\n",
    "        # (\n",
    "        #     (\n",
    "        #         TokenID,\n",
    "        #         CameraPerspective\n",
    "        #     ),\n",
    "        #     listof(\n",
    "        #       (ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence)\n",
    "        #     )\n",
    "        # )\n",
    ")\n",
    "\n",
    "\n",
    "def flatten_ctvusts_by_tcp(ctvusts_by_tcp_tpl):\n",
    "    return [\n",
    "        (\n",
    "            ctvusts_by_tcp_tpl[0][0],   # TokenID\n",
    "            ctvusts_by_tcp_tpl[0][1],   # CameraPerspective\n",
    "            ctvusts_tpl[0],             # ASLConsultantID\n",
    "            ctvusts_tpl[1],             # TargetVideoFilename\n",
    "            ctvusts_tpl[2],             # UtteranceSequence\n",
    "            ctvusts_tpl[3]              # TokenSequence\n",
    "        ) for ctvusts_tpl in ctvusts_by_tcp_tpl[1]\n",
    "    ]\n",
    "\n",
    "ctvusts_by_tcp__gt_1 = (\n",
    "    ctvusts_by_tcp\n",
    "    | \"Beam PL: filter candidate (TokenID,CameraPerspective) for test-validation split\" >> beam.Filter(\n",
    "            lambda list_ctvusts_by_tcp_tpl: len(set(list_ctvusts_by_tcp_tpl[1])) > 1\n",
    "        )\n",
    "    | \"Beam PL: flatten filtered (TokenID,CameraPerspective) candidates for test-validation split\" >> beam.FlatMap(flatten_ctvusts_by_tcp)\n",
    ")\n",
    "\n",
    "ctvusts_by_tcp__lte_1 = (\n",
    "    ctvusts_by_tcp\n",
    "    | \"Beam PL: filter non-candidate (TokenID,CameraPerspective) for test-validation split\" >> beam.Filter(\n",
    "            lambda list_ctvusts_by_tcp_tpl: len(set(list_ctvusts_by_tcp_tpl[1])) <= 1\n",
    "        )\n",
    "    | \"Beam PL: flatten filtered (TokenID,CameraPerspective) non-candidates for test-validation split\" >> beam.FlatMap(flatten_ctvusts_by_tcp)\n",
    ")"
   ]
  },
  {
   "source": [
    "<p><br>\n",
    "\n",
    "#### Finally, execute validation/train split on ctvusts_by_tcp__gt_1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to put ctvusts_by_tcp__gt_1 back into ((TokenID, CameraPerspective), (ASLConsultantID, TargetVideoFilename, UtteranceSequence, TokenSequence)) form\n",
    "def rekey_ctvusts_by_tcp(ctvusts_by_tcp_tpl):\n",
    "    return (\n",
    "        (\n",
    "            ctvusts_by_tcp_tpl[0],  # TokenID\n",
    "            ctvusts_by_tcp_tpl[1]   # CameraPerspective\n",
    "        ),\n",
    "        (\n",
    "            ctvusts_by_tcp_tpl[2],  # ASLConsultantID\n",
    "            ctvusts_by_tcp_tpl[3],  # TargetVideoFilename\n",
    "            ctvusts_by_tcp_tpl[4],  # UtteranceSequence\n",
    "            ctvusts_by_tcp_tpl[5]   # TokenSequence\n",
    "        )\n",
    "    )\n",
    "\n",
    "def val_train_split__ctvusts_by_tcp__gt_1__tpl(ctvusts_list__by__tcp__gt_1__tpl):\n",
    "    \"\"\"\n",
    "    ctvusts_list__by__tcp__gt_1__tpl\n",
    "        (\n",
    "            (TokenID,CameraPerspective), # key\n",
    "            listof(\n",
    "                (ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence)\n",
    "            )\n",
    "        )\n",
    "    \"\"\"\n",
    "    ctvusts_list = ctvusts_list__by__tcp__gt_1__tpl[1].copy() # we need a copy since we want to shuffle\n",
    "    random.shuffle(ctvusts_list)\n",
    "    len_ctvusts_list = len(ctvusts_list)\n",
    "    val_len_ctvusts_list = int(len_ctvusts_list*fidscs_globals.VALIDATION_SIZE_RATIO) if len_ctvusts_list > int(((1-fidscs_globals.VALIDATION_SIZE_RATIO)*100)/10) else 1\n",
    "    train__ctvusts_list, val__ctvusts_list = ctvusts_list[val_len_ctvusts_list:], ctvusts_list[:val_len_ctvusts_list]\n",
    "    return (\n",
    "        (\n",
    "            ctvusts_list__by__tcp__gt_1__tpl[0][0],    # TokenID\n",
    "            ctvusts_list__by__tcp__gt_1__tpl[0][1]     # CameraPerspective\n",
    "        ),\n",
    "        (\n",
    "            train__ctvusts_list,\n",
    "            val__ctvusts_list\n",
    "        )\n",
    "    )\n",
    "\n",
    "val_train_split_basis__ctvusts_by_tcp__gt_1 = (\n",
    "    ctvusts_by_tcp__gt_1\n",
    "    | \"Beam PL: rekey ctvusts_by_tcp__gt_1 for validation/train split\" >> beam.Map(rekey_ctvusts_by_tcp)\n",
    "    | \"Beam PL: group (ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence) rekeyed by (TokenID,CameraPerspective)\" >> beam.GroupByKey()\n",
    "    # the above produces tuples of the form:\n",
    "        # (\n",
    "        #     (TokenID,CameraPerspective), # key\n",
    "        #     listof(\n",
    "        #       (ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence)\n",
    "        #     )\n",
    "        # )\n",
    "    | \"Beam PL: split rekeyed ctvusts_list_by_tcp__gt_1\" >> beam.Map(val_train_split__ctvusts_by_tcp__gt_1__tpl)\n",
    "    # the above produces tuples of the form:\n",
    "        # (\n",
    "        #     (TokenID,CameraPerspective), # key\n",
    "        #     (\n",
    "        #       test_list_of(ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence),\n",
    "        #       val_list_of(ASLConsultantID,TargetVideoFilename,UtteranceSequence,TokenSequence),\n",
    "        #     )\n",
    "        # )\n",
    ")\n",
    "\n",
    "train__ctvusts_by_tcp__gt_1 = (\n",
    "    val_train_split_basis__ctvusts_by_tcp__gt_1\n",
    "    | \"Beam PL: select train sublist from val_train_split_basis__ctvusts_by_tcp__gt_1\" >> beam.Map(\n",
    "            lambda val_train_split_basis__ctvusts_by_tcp__gt_1_tpl: [\n",
    "                (\n",
    "                    val_train_split_basis__ctvusts_by_tcp__gt_1_tpl[0][0],  # TokenID\n",
    "                    val_train_split_basis__ctvusts_by_tcp__gt_1_tpl[0][1],  # CameraPerspective\n",
    "                    train_ctvusts_tpl[0],                                   # ASLConsultantID\n",
    "                    train_ctvusts_tpl[1],                                   # TargetVideoFilename\n",
    "                    train_ctvusts_tpl[2],                                   # UtteranceSequence\n",
    "                    train_ctvusts_tpl[3],                                   # UtteranceSequence\n",
    "                ) for train_ctvusts_tpl in val_train_split_basis__ctvusts_by_tcp__gt_1_tpl[1][0] # index [1][0] points to train sublist\n",
    "            ]\n",
    "        )\n",
    "    | \"Beam PL: 'explode list_train__ctvusts_by_tcp__gt_1_tpl\" >> beam.FlatMap(lambda list_train__ctvusts_by_tcp__gt_1_tpl: list_train__ctvusts_by_tcp__gt_1_tpl)\n",
    ")\n",
    "\n",
    "val__ctvusts_by_tcp__gt_1 = (\n",
    "    val_train_split_basis__ctvusts_by_tcp__gt_1\n",
    "    | \"Beam PL: select validation sublist from val_train_split_basis__ctvusts_by_tcp__gt_1\" >> beam.Map(\n",
    "            lambda val_train_split_basis__ctvusts_by_tcp__gt_1_tpl: [\n",
    "                (\n",
    "                    val_train_split_basis__ctvusts_by_tcp__gt_1_tpl[0][0],  # TokenID\n",
    "                    val_train_split_basis__ctvusts_by_tcp__gt_1_tpl[0][1],  # CameraPerspective\n",
    "                    val_ctvusts_tpl[0],                                     # ASLConsultantID\n",
    "                    val_ctvusts_tpl[1],                                     # TargetVideoFilename\n",
    "                    val_ctvusts_tpl[2],                                     # UtteranceSequence\n",
    "                    val_ctvusts_tpl[3],                                     # UtteranceSequence\n",
    "                ) for val_ctvusts_tpl in val_train_split_basis__ctvusts_by_tcp__gt_1_tpl[1][1] # index [1][1] points to validation sublist\n",
    "            ]\n",
    "        )\n",
    "    | \"Beam PL: 'explode list_val__ctvusts_by_tcp__gt_1_tpl\" >> beam.FlatMap(lambda list_val__ctvusts_by_tcp__gt_1_tpl: list_val__ctvusts_by_tcp__gt_1_tpl)\n",
    ")\n",
    "\n",
    "# train_supp__ctvusts_by_tcp__lte_1 = (\n",
    "#     ctvusts_by_tcp__lte_1\n",
    "#     | \"Beam PL: rekey ctvusts_by_tcp__lte_1 for supplemental train\" >> beam.Map(rekey_ctvusts_by_tcp)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we require this in order to make use of ib.show() (which provides visualization of the pcolls specified) or ib.collect() (which creates a pandas dataframe from a pcoll)\n",
    "    # but all pcolls we wish to visualize must be created prior to executing the following line\n",
    "ib.watch(locals())"
   ]
  },
  {
   "source": [
    "#### Show those with counts > 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n            <div id=\"progress_indicator_a8942b869772f43c29843b8cf82436b7\" class=\"spinner-border text-info\" role=\"status\">\n            </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n            $(\"#progress_indicator_a8942b869772f43c29843b8cf82436b7\").remove();\n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n            $(\"#progress_indicator_a8942b869772f43c29843b8cf82436b7\").remove();\n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_ctvusts_by_tcp__gt_1 = ib.collect(ctvusts_by_tcp__gt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           ASLConsultantID   TargetVideoFilename  \\\n",
       "TokenID CameraPerspective                                          \n",
       "0       0                                1   DSP%2520DeadDog.mov   \n",
       "        0                                3  d18_1149_small_0.mov   \n",
       "        0                                4     _1337_small_0.mov   \n",
       "        0                                4     _1339_small_0.mov   \n",
       "        0                                4     _1339_small_0.mov   \n",
       "...                                    ...                   ...   \n",
       "2396    0                                4     _1449_small_0.mov   \n",
       "        1                                4     _1441_small_1.mov   \n",
       "        1                                4     _1449_small_1.mov   \n",
       "        3                                4     _1441_small_3.mov   \n",
       "        3                                4     _1449_small_3.mov   \n",
       "\n",
       "                           UtteranceSequence  TokenSequence  \n",
       "TokenID CameraPerspective                                    \n",
       "0       0                                 11              2  \n",
       "        0                                  5              0  \n",
       "        0                                 42              0  \n",
       "        0                                 44              0  \n",
       "        0                                 44              4  \n",
       "...                                      ...            ...  \n",
       "2396    0                                151              4  \n",
       "        1                                143              5  \n",
       "        1                                151              4  \n",
       "        3                                143              5  \n",
       "        3                                151              4  \n",
       "\n",
       "[29762 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th>0</th>\n      <td>1</td>\n      <td>DSP%2520DeadDog.mov</td>\n      <td>11</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>d18_1149_small_0.mov</td>\n      <td>5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>_1337_small_0.mov</td>\n      <td>42</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>_1339_small_0.mov</td>\n      <td>44</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>_1339_small_0.mov</td>\n      <td>44</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2396</th>\n      <th>0</th>\n      <td>4</td>\n      <td>_1449_small_0.mov</td>\n      <td>151</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>_1441_small_1.mov</td>\n      <td>143</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>_1449_small_1.mov</td>\n      <td>151</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>_1441_small_3.mov</td>\n      <td>143</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>_1449_small_3.mov</td>\n      <td>151</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>29762 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df_ctvusts_by_tcp__gt_1.columns = ['TokenID', 'CameraPerspective', 'ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence']\n",
    "df_ctvusts_by_tcp__gt_1.set_index(['TokenID', 'CameraPerspective'], inplace=True)\n",
    "df_ctvusts_by_tcp__gt_1.sort_values(axis=0, by=['ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence'], ignore_index=False, inplace=True)\n",
    "df_ctvusts_by_tcp__gt_1.sort_index(inplace=True)\n",
    "df_ctvusts_by_tcp__gt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ctvusts_by_tcp__gt_1.loc[\n",
    "#     (\n",
    "#         [2369],         # TokenID\n",
    "#         [2]             # CameraPerspective\n",
    "#     ), \n",
    "#     :\n",
    "# ].sort_index(ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           count\n",
       "TokenID CameraPerspective       \n",
       "906     0                    694\n",
       "0       0                    629\n",
       "906     2                    570\n",
       "0       2                    559\n",
       "318     0                    414\n",
       "...                          ...\n",
       "316     3                      2\n",
       "        2                      2\n",
       "        0                      2\n",
       "588     0                      2\n",
       "2396    3                      2\n",
       "\n",
       "[3079 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>count</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>906</th>\n      <th>0</th>\n      <td>694</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th>0</th>\n      <td>629</td>\n    </tr>\n    <tr>\n      <th>906</th>\n      <th>2</th>\n      <td>570</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th>2</th>\n      <td>559</td>\n    </tr>\n    <tr>\n      <th>318</th>\n      <th>0</th>\n      <td>414</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">316</th>\n      <th>3</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>588</th>\n      <th>0</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2396</th>\n      <th>3</th>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>3079 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "df_ctvusts_by_tcp__gt_1__count = df_ctvusts_by_tcp__gt_1.reset_index().groupby(['TokenID', 'CameraPerspective']).count()\n",
    "df_ctvusts_by_tcp__gt_1__count = df_ctvusts_by_tcp__gt_1__count[['ASLConsultantID']]\n",
    "df_ctvusts_by_tcp__gt_1__count.columns = ['count']\n",
    "df_ctvusts_by_tcp__gt_1__count.sort_values(axis=0, by=['count'], ascending=False, inplace=True)\n",
    "# df_ctvusts_by_tcp__gt_1__count.sort_index(inplace=True)\n",
    "df_ctvusts_by_tcp__gt_1__count"
   ]
  },
  {
   "source": [
    "#### Now show those with counts <= 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n            <div id=\"progress_indicator_0516fe247a6ef42428eaff04dbfafe31\" class=\"spinner-border text-info\" role=\"status\">\n            </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n            $(\"#progress_indicator_0516fe247a6ef42428eaff04dbfafe31\").remove();\n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n            $(\"#progress_indicator_0516fe247a6ef42428eaff04dbfafe31\").remove();\n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_ctvusts_by_tcp__lte_1 = ib.collect(ctvusts_by_tcp__lte_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           ASLConsultantID           TargetVideoFilename  \\\n",
       "TokenID CameraPerspective                                                  \n",
       "1       0                                6     ben_story_439_small_0.mov   \n",
       "        1                                6     ben_story_439_small_1.mov   \n",
       "        2                                6     ben_story_439_small_2.mov   \n",
       "        3                                6     ben_story_439_small_3.mov   \n",
       "2       0                                6     ben_story_439_small_0.mov   \n",
       "...                                    ...                           ...   \n",
       "2409    0                                0        DSP%2520Immigrants.mov   \n",
       "2410    0                                4    boston-la_1088_small_0.mov   \n",
       "        2                                4    boston-la_1088_small_2.mov   \n",
       "2411    0                                4  biker_buddy_1069_small_0.mov   \n",
       "        2                                4  biker_buddy_1069_small_2.mov   \n",
       "\n",
       "                           UtteranceSequence  TokenSequence  \n",
       "TokenID CameraPerspective                                    \n",
       "1       0                                 27              4  \n",
       "        1                                 27              4  \n",
       "        2                                 27              4  \n",
       "        3                                 27              4  \n",
       "2       0                                 45              3  \n",
       "...                                      ...            ...  \n",
       "2409    0                                 15              4  \n",
       "2410    0                                 35              3  \n",
       "        2                                 35              3  \n",
       "2411    0                                  9              3  \n",
       "        2                                  9              3  \n",
       "\n",
       "[3691 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">1</th>\n      <th>0</th>\n      <td>6</td>\n      <td>ben_story_439_small_0.mov</td>\n      <td>27</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>ben_story_439_small_1.mov</td>\n      <td>27</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>ben_story_439_small_2.mov</td>\n      <td>27</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>ben_story_439_small_3.mov</td>\n      <td>27</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <th>0</th>\n      <td>6</td>\n      <td>ben_story_439_small_0.mov</td>\n      <td>45</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2409</th>\n      <th>0</th>\n      <td>0</td>\n      <td>DSP%2520Immigrants.mov</td>\n      <td>15</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">2410</th>\n      <th>0</th>\n      <td>4</td>\n      <td>boston-la_1088_small_0.mov</td>\n      <td>35</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>boston-la_1088_small_2.mov</td>\n      <td>35</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">2411</th>\n      <th>0</th>\n      <td>4</td>\n      <td>biker_buddy_1069_small_0.mov</td>\n      <td>9</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>biker_buddy_1069_small_2.mov</td>\n      <td>9</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>3691 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df_ctvusts_by_tcp__lte_1.columns = ['TokenID', 'CameraPerspective', 'ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence']\n",
    "df_ctvusts_by_tcp__lte_1.set_index(['TokenID', 'CameraPerspective'], inplace=True)\n",
    "df_ctvusts_by_tcp__lte_1.sort_values(axis=0, by=['ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence'], ignore_index=False, inplace=True)\n",
    "df_ctvusts_by_tcp__lte_1.sort_index(inplace=True)\n",
    "df_ctvusts_by_tcp__lte_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           count\n",
       "TokenID CameraPerspective       \n",
       "1       0                      1\n",
       "1621    0                      1\n",
       "1626    0                      1\n",
       "        2                      1\n",
       "1631    0                      1\n",
       "...                          ...\n",
       "828     0                      1\n",
       "        2                      1\n",
       "829     0                      1\n",
       "        2                      1\n",
       "2411    2                      1\n",
       "\n",
       "[3691 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>count</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1621</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">1626</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1631</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">828</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">829</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2411</th>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3691 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df_ctvusts_by_tcp__lte_1__count = df_ctvusts_by_tcp__lte_1.reset_index().groupby(['TokenID', 'CameraPerspective']).count()\n",
    "df_ctvusts_by_tcp__lte_1__count = df_ctvusts_by_tcp__lte_1__count[['ASLConsultantID']]\n",
    "df_ctvusts_by_tcp__lte_1__count.columns = ['count']\n",
    "df_ctvusts_by_tcp__lte_1__count.sort_values(axis=0, by=['count'], ascending=False, inplace=True)\n",
    "# df_ctvusts_by_tcp__gt_1__count.sort_index(inplace=True)\n",
    "df_ctvusts_by_tcp__lte_1__count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ASLConsultantID_left, TargetVideoFilename_left, UtteranceSequence_left, TokenSequence_left, ASLConsultantID_right, TargetVideoFilename_right, UtteranceSequence_right, TokenSequence_right]\n",
       "Index: []"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID_left</th>\n      <th>TargetVideoFilename_left</th>\n      <th>UtteranceSequence_left</th>\n      <th>TokenSequence_left</th>\n      <th>ASLConsultantID_right</th>\n      <th>TargetVideoFilename_right</th>\n      <th>UtteranceSequence_right</th>\n      <th>TokenSequence_right</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "df_ctvusts_by_tcp__intersection = df_ctvusts_by_tcp__gt_1.join(df_ctvusts_by_tcp__lte_1, how='inner', lsuffix='_left', rsuffix='_right')\n",
    "df_ctvusts_by_tcp__intersection"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Now show train/validation split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n            <div id=\"progress_indicator_e7e01dffa39fd5d9cad87c7635bfa4ee\" class=\"spinner-border text-info\" role=\"status\">\n            </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n            $(\"#progress_indicator_e7e01dffa39fd5d9cad87c7635bfa4ee\").remove();\n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n            $(\"#progress_indicator_e7e01dffa39fd5d9cad87c7635bfa4ee\").remove();\n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_train__ctvusts_by_tcp__gt_1 = ib.collect(train__ctvusts_by_tcp__gt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           ASLConsultantID          TargetVideoFilename  \\\n",
       "TokenID CameraPerspective                                                 \n",
       "0       0                                1          DSP%2520DeadDog.mov   \n",
       "        0                                3         d18_1149_small_0.mov   \n",
       "        0                                4            _1337_small_0.mov   \n",
       "        0                                4            _1339_small_0.mov   \n",
       "        0                                4            _1344_small_0.mov   \n",
       "...                                    ...                          ...   \n",
       "2394    2                                4  whitewater_1049_small_2.mov   \n",
       "        3                                4  whitewater_1049_small_3.mov   \n",
       "2396    0                                4            _1449_small_0.mov   \n",
       "        1                                4            _1441_small_1.mov   \n",
       "        3                                4            _1449_small_3.mov   \n",
       "\n",
       "                           UtteranceSequence  TokenSequence  \n",
       "TokenID CameraPerspective                                    \n",
       "0       0                                 11              2  \n",
       "        0                                  5              0  \n",
       "        0                                 42              0  \n",
       "        0                                 44              0  \n",
       "        0                                 48              0  \n",
       "...                                      ...            ...  \n",
       "2394    2                                 12              2  \n",
       "        3                                 40              4  \n",
       "2396    0                                151              4  \n",
       "        1                                143              5  \n",
       "        3                                151              4  \n",
       "\n",
       "[25381 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th>0</th>\n      <td>1</td>\n      <td>DSP%2520DeadDog.mov</td>\n      <td>11</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>d18_1149_small_0.mov</td>\n      <td>5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>_1337_small_0.mov</td>\n      <td>42</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>_1339_small_0.mov</td>\n      <td>44</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>_1344_small_0.mov</td>\n      <td>48</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">2394</th>\n      <th>2</th>\n      <td>4</td>\n      <td>whitewater_1049_small_2.mov</td>\n      <td>12</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>whitewater_1049_small_3.mov</td>\n      <td>40</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">2396</th>\n      <th>0</th>\n      <td>4</td>\n      <td>_1449_small_0.mov</td>\n      <td>151</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>_1441_small_1.mov</td>\n      <td>143</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>_1449_small_3.mov</td>\n      <td>151</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>25381 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "df_train__ctvusts_by_tcp__gt_1.columns = ['TokenID', 'CameraPerspective', 'ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence']\n",
    "df_train__ctvusts_by_tcp__gt_1.set_index(['TokenID', 'CameraPerspective'], inplace=True)\n",
    "df_train__ctvusts_by_tcp__gt_1.sort_values(axis=0, by=['ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence'], ignore_index=False, inplace=True)\n",
    "df_train__ctvusts_by_tcp__gt_1.sort_index(inplace=True)\n",
    "df_train__ctvusts_by_tcp__gt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n            <div id=\"progress_indicator_4267444cf2619c15dc31adee6a5380d4\" class=\"spinner-border text-info\" role=\"status\">\n            </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n            $(\"#progress_indicator_4267444cf2619c15dc31adee6a5380d4\").remove();\n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n            $(\"#progress_indicator_4267444cf2619c15dc31adee6a5380d4\").remove();\n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_val__ctvusts_by_tcp__gt_1 = ib.collect(val__ctvusts_by_tcp__gt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           ASLConsultantID          TargetVideoFilename  \\\n",
       "TokenID CameraPerspective                                                 \n",
       "0       0                                4            _1339_small_0.mov   \n",
       "        0                                4            _1365_small_0.mov   \n",
       "        0                                4            _1400_small_0.mov   \n",
       "        0                                4            _1442_small_0.mov   \n",
       "        0                                4            _1445_small_0.mov   \n",
       "...                                    ...                          ...   \n",
       "2394    2                                4  whitewater_1049_small_2.mov   \n",
       "        3                                4  whitewater_1049_small_3.mov   \n",
       "2396    0                                4            _1441_small_0.mov   \n",
       "        1                                4            _1449_small_1.mov   \n",
       "        3                                4            _1441_small_3.mov   \n",
       "\n",
       "                           UtteranceSequence  TokenSequence  \n",
       "TokenID CameraPerspective                                    \n",
       "0       0                                 44              4  \n",
       "        0                                 69              0  \n",
       "        0                                102              0  \n",
       "        0                                144              1  \n",
       "        0                                147              3  \n",
       "...                                      ...            ...  \n",
       "2394    2                                 40              4  \n",
       "        3                                 12              2  \n",
       "2396    0                                143              5  \n",
       "        1                                151              4  \n",
       "        3                                143              5  \n",
       "\n",
       "[4381 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th>0</th>\n      <td>4</td>\n      <td>_1339_small_0.mov</td>\n      <td>44</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>_1365_small_0.mov</td>\n      <td>69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>_1400_small_0.mov</td>\n      <td>102</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>_1442_small_0.mov</td>\n      <td>144</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>_1445_small_0.mov</td>\n      <td>147</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">2394</th>\n      <th>2</th>\n      <td>4</td>\n      <td>whitewater_1049_small_2.mov</td>\n      <td>40</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>whitewater_1049_small_3.mov</td>\n      <td>12</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">2396</th>\n      <th>0</th>\n      <td>4</td>\n      <td>_1441_small_0.mov</td>\n      <td>143</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>_1449_small_1.mov</td>\n      <td>151</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>_1441_small_3.mov</td>\n      <td>143</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>4381 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "df_val__ctvusts_by_tcp__gt_1.columns = ['TokenID', 'CameraPerspective', 'ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence']\n",
    "df_val__ctvusts_by_tcp__gt_1.set_index(['TokenID', 'CameraPerspective'], inplace=True)\n",
    "df_val__ctvusts_by_tcp__gt_1.sort_values(axis=0, by=['ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence'], ignore_index=False, inplace=True)\n",
    "df_val__ctvusts_by_tcp__gt_1.sort_index(inplace=True)\n",
    "df_val__ctvusts_by_tcp__gt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           ASLConsultantID          TargetVideoFilename  \\\n",
       "TokenID CameraPerspective                                                 \n",
       "2394    2                                4  whitewater_1049_small_2.mov   \n",
       "\n",
       "                           UtteranceSequence  TokenSequence  \n",
       "TokenID CameraPerspective                                    \n",
       "2394    2                                 12              2  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2394</th>\n      <th>2</th>\n      <td>4</td>\n      <td>whitewater_1049_small_2.mov</td>\n      <td>12</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "df_train__ctvusts_by_tcp__gt_1.loc[\n",
    "    (\n",
    "        [2394],         # TokenID\n",
    "        [2]             # CameraPerspective\n",
    "    ), \n",
    "    :\n",
    "].sort_index(ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           ASLConsultantID          TargetVideoFilename  \\\n",
       "TokenID CameraPerspective                                                 \n",
       "2394    2                                4  whitewater_1049_small_2.mov   \n",
       "\n",
       "                           UtteranceSequence  TokenSequence  \n",
       "TokenID CameraPerspective                                    \n",
       "2394    2                                 40              4  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2394</th>\n      <th>2</th>\n      <td>4</td>\n      <td>whitewater_1049_small_2.mov</td>\n      <td>40</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "df_val__ctvusts_by_tcp__gt_1.loc[\n",
    "    (\n",
    "        [2394],         # TokenID\n",
    "        [2]             # CameraPerspective\n",
    "    ), \n",
    "    :\n",
    "].sort_index(ascending=[True, True])"
   ]
  }
 ]
}