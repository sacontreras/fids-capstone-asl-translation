#!/bin/bash

# Copyright 2019 Google Inc. All Rights Reserved. Licensed under the Apache
# License, Version 2.0 (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.

set -e

# Parse command line arguments
unset WORK_DIR
SRC_DIR=.
WORK_DIR=gs://sc-fids-capstone-bucket/fids-capstone-data
MAX_TARGET_VIDEOS=5
USE_BEAM=1
BEAM_RUNNER=DataflowRunner
BEAM_GCP_PROJECT=$(gcloud config get-value project || echo $PROJECT)
REGION=us-central1
while [[ $# -gt 0 ]]; do
  case $1 in
    --src-dir)
      SRC_DIR=$2
      shift
      ;;
    --work-dir)
      WORK_DIR=$2
      shift
      ;;
    --max-target-videos)
      MAX_TARGET_VIDEOS=$2
      shift
      ;;
    # --use-beam)
    #   USE_BEAM=$2 #${2:-1}
    #   shift
    #   ;;
    --beam-runner)  # in case we want to run on AWS or someplace other than GCP Dataflow
      BEAM_RUNNER=$2
      shift
      ;;
    --beam-gcp-project) # this is retrieved automatically via the script
      BEAM_RUNNER=$2
      shift
      ;;
    *)
      echo "error: unrecognized argument $1"
      exit 1
      ;;
  esac
  shift
done

if [[ -z $WORK_DIR ]]; then
  echo "error: argument --work-dir is required"
  exit 1
fi

if [[ $WORK_DIR != gs://* ]]; then
  echo "error: --work-dir must be a Google Cloud Storage path"
  echo "       example: gs://sc-fids-capstone-bucket/fids-capstone-data"
  exit 1
fi

if [[ -z $PROJECT ]]; then
  echo 'error: --project is required to run in Google Cloud Platform.'
  exit 1
fi

# Wrapper function to print the command being run
function run {
  echo "$ $@"
  "$@"
}

# Extract the data files
echo '>>>>>>>>>>>>>>>>>>>>>>>>> Extracting data <<<<<<<<<<<<<<<<<<<<<<<<<'
run python $SRC_DIR/data_extractor.py \
  --work-dir $WORK_DIR \
  --max-target-videos $MAX_TARGET_VIDEOS \
  --use-beam $USE_BEAM \
  --beam-runner $BEAM_RUNNER \
  --beam-gcp-project $BEAM_GCP_PROJECT \
  --beam-gcs-temp_location $WORK_DIR/beam-temp
echo ''

# Missing required option: region.
# Missing GCS path option: temp_location.
# Missing GCS path option: staging_location.

# Preprocess the datasets
echo '>>>>>>>>>>>>>>>>>>>>>>>>> Preprocessing <<<<<<<<<<<<<<<<<<<<<<<<<'
run python preprocessor.py \
  --work-dir $WORK_DIR \
  --beam-gcp-project $PROJECT \
  --beam-gcs-temp_location $WORK_DIR/beam-temp \
echo ''

# # Train and evaluate the model
# echo '>>>>>>>>>>>>>>>>>>>>>>>>> Training <<<<<<<<<<<<<<<<<<<<<<<<<'
# run python train.py \
#   --work-dir $WORK_DIR
# echo ''

# # Get the model path
# EXPORT_DIR=$WORK_DIR/model/export/final
# if [[ $EXPORT_DIR == gs://* ]]; then
#   MODEL_DIR=$(gsutil ls -d "$EXPORT_DIR/*" | sort -r | head -n 1)
# else
#   MODEL_DIR=$(ls -d -1 $EXPORT_DIR/* | sort -r | head -n 1)
# fi
# echo "Model: $MODEL_DIR"
# echo ''

# # Make batch predictions on SDF files
# echo '>> Batch prediction'
# run python predict.py \
#   --work-dir $WORK_DIR \
#   --model-dir $MODEL_DIR \
#   batch \
#   --inputs-dir $WORK_DIR/data \
#   --outputs-dir $WORK_DIR/predictions

# # Display some predictions
# if [[ $WORK_DIR == gs://* ]]; then
#   gsutil cat $WORK_DIR/predictions/* | head -n 10
# else
#   head -n 10 $WORK_DIR/predictions/*
# fi