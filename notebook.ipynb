{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.4 64-bit ('learn-env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "0939b78ba6faff4474cfb5060aa28f303b42bca067e2d3896f1922bb73ddb2d6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "import apache_beam.transforms.sql\n",
    "\n",
    "import beam__common\n",
    "import fidscs_globals\n",
    "\n",
    "from importlib import import_module\n",
    "data_extractor = import_module('data-extractor', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "use_beam: True\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CollectiveCommunication.AUTO\n",
      "Number of devices available for parallel processing: 1\n",
      "PipelineOptions:\n",
      "{'runner': 'DirectRunner', 'streaming': False, 'beam_services': {}, 'type_check_strictness': 'DEFAULT_TO_ANY', 'type_check_additional': '', 'pipeline_type_check': True, 'runtime_type_check': False, 'performance_runtime_type_check': False, 'direct_runner_use_stacked_bundle': True, 'direct_runner_bundle_repeat': 0, 'direct_num_workers': 0, 'direct_running_mode': 'multi_threading', 'dataflow_endpoint': 'https://dataflow.googleapis.com', 'project': 'my-project', 'job_name': None, 'staging_location': None, 'temp_location': None, 'region': None, 'service_account_email': None, 'no_auth': False, 'template_location': None, 'labels': None, 'update': False, 'transform_name_mapping': None, 'enable_streaming_engine': False, 'dataflow_kms_key': None, 'flexrs_goal': None, 'hdfs_host': None, 'hdfs_port': None, 'hdfs_user': None, 'hdfs_full_urls': False, 'num_workers': None, 'max_num_workers': None, 'autoscaling_algorithm': None, 'machine_type': None, 'disk_size_gb': None, 'disk_type': None, 'worker_region': None, 'worker_zone': None, 'zone': None, 'network': None, 'subnetwork': None, 'worker_harness_container_image': None, 'sdk_harness_container_image_overrides': None, 'use_public_ips': None, 'min_cpu_platform': None, 'dataflow_worker_jar': None, 'dataflow_job_file': None, 'experiments': None, 'number_of_worker_harness_threads': None, 'profile_cpu': False, 'profile_memory': False, 'profile_location': None, 'profile_sample_rate': 1.0, 'requirements_file': None, 'requirements_cache': None, 'setup_file': None, 'beam_plugins': None, 'save_main_session': False, 'sdk_location': 'default', 'extra_packages': None, 'prebuild_sdk_container_engine': None, 'prebuild_sdk_container_base_image': None, 'docker_registry_push_url': None, 'job_endpoint': None, 'artifact_endpoint': None, 'job_server_timeout': 60, 'environment_type': None, 'environment_config': None, 'environment_options': None, 'sdk_worker_parallelism': 1, 'environment_cache_millis': 0, 'output_executable_path': None, 'artifacts_dir': None, 'job_port': 0, 'artifact_port': 0, 'expansion_port': 0, 'flink_master': '[auto]', 'flink_version': '1.10', 'flink_job_server_jar': None, 'flink_submit_uber_jar': False, 'spark_master_url': 'local[4]', 'spark_job_server_jar': None, 'spark_submit_uber_jar': False, 'spark_rest_url': None, 'on_success_matcher': None, 'dry_run': False, 'wait_until_finish_duration': None, 'pubsubRootUrl': None, 's3_access_key_id': None, 's3_secret_access_key': None, 's3_session_token': None, 's3_endpoint_url': None, 's3_region_name': None, 's3_api_version': None, 's3_verify': None, 's3_disable_ssl': False}\n",
      "\n",
      "Found dataset /tmp/fids-capstone-data/data/consultant-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-targetvideo-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-utterance-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-utterance-targetvideo-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-utterance-token-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/ncslgr-corpus-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/document-consultant-targetvideo-utterance-token-frame-index.csv\n",
      "Found dataset /tmp/fids-capstone-data/data/vocabulary-index.csv\n",
      "Beam PL: ALL DONE!\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/tmp/fids-capstone-data/data\"\n",
    "\n",
    "data_extractor.run(max_data_files=-1, data_dir=data_dir, use_beam=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PipelineOptions:\n{'runner': 'InteractiveRunner', 'streaming': False, 'beam_services': {}, 'type_check_strictness': 'DEFAULT_TO_ANY', 'type_check_additional': '', 'pipeline_type_check': True, 'runtime_type_check': False, 'performance_runtime_type_check': False, 'direct_runner_use_stacked_bundle': True, 'direct_runner_bundle_repeat': 0, 'direct_num_workers': 0, 'direct_running_mode': 'multi_threading', 'dataflow_endpoint': 'https://dataflow.googleapis.com', 'project': 'my-project', 'job_name': None, 'staging_location': None, 'temp_location': None, 'region': None, 'service_account_email': None, 'no_auth': False, 'template_location': None, 'labels': None, 'update': False, 'transform_name_mapping': None, 'enable_streaming_engine': False, 'dataflow_kms_key': None, 'flexrs_goal': None, 'hdfs_host': None, 'hdfs_port': None, 'hdfs_user': None, 'hdfs_full_urls': False, 'num_workers': None, 'max_num_workers': None, 'autoscaling_algorithm': None, 'machine_type': None, 'disk_size_gb': None, 'disk_type': None, 'worker_region': None, 'worker_zone': None, 'zone': None, 'network': None, 'subnetwork': None, 'worker_harness_container_image': None, 'sdk_harness_container_image_overrides': None, 'use_public_ips': None, 'min_cpu_platform': None, 'dataflow_worker_jar': None, 'dataflow_job_file': None, 'experiments': None, 'number_of_worker_harness_threads': None, 'profile_cpu': False, 'profile_memory': False, 'profile_location': None, 'profile_sample_rate': 1.0, 'requirements_file': None, 'requirements_cache': None, 'setup_file': None, 'beam_plugins': None, 'save_main_session': False, 'sdk_location': 'default', 'extra_packages': None, 'prebuild_sdk_container_engine': None, 'prebuild_sdk_container_base_image': None, 'docker_registry_push_url': None, 'job_endpoint': None, 'artifact_endpoint': None, 'job_server_timeout': 60, 'environment_type': None, 'environment_config': None, 'environment_options': None, 'sdk_worker_parallelism': 1, 'environment_cache_millis': 0, 'output_executable_path': None, 'artifacts_dir': None, 'job_port': 0, 'artifact_port': 0, 'expansion_port': 0, 'flink_master': '[auto]', 'flink_version': '1.10', 'flink_job_server_jar': None, 'flink_submit_uber_jar': False, 'spark_master_url': 'local[4]', 'spark_job_server_jar': None, 'spark_submit_uber_jar': False, 'spark_rest_url': None, 'on_success_matcher': None, 'dry_run': False, 'wait_until_finish_duration': None, 'pubsubRootUrl': None, 's3_access_key_id': None, 's3_secret_access_key': None, 's3_session_token': None, 's3_endpoint_url': None, 's3_region_name': None, 's3_api_version': None, 's3_verify': None, 's3_disable_ssl': False}\n\n"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "    'project': 'my-project', # change\n",
    "    # 'runner': 'DirectRunner',\n",
    "    'runner': 'InteractiveRunner',\n",
    "    'direct_num_workers': 0, # 0 is use all available cores\n",
    "    'direct_running_mode': 'multi_threading', # ['in_memory', 'multi_threading', 'multi_processing'] # 'multi_processing' doesn't seem to work for DirectRunner?\n",
    "    'streaming': False # set to True if data source is unbounded (e.g. GCP PubSub)\n",
    "}\n",
    "pipeline_options = PipelineOptions(flags=[], **options) # easier to pass in options from command-line this way\n",
    "print(f\"PipelineOptions:\\n{pipeline_options.get_all_options()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidscs_globals.DATA_ROOT_DIR = data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "can_proceed = True\n",
    "\n",
    "if not tf.io.gfile.exists(fidscs_globals.DATA_ROOT_DIR) or len(tf.io.gfile.listdir(fidscs_globals.DATA_ROOT_DIR))==0:\n",
    "    print(f\"{fidscs_globals.VALIDATION_FATAL_ERROR_TEXT} data directory does not exist or is empty!\")\n",
    "    can_proceed = False\n",
    "else:\n",
    "    fidscs_globals.VIDEO_DIR = os.path.join(fidscs_globals.DATA_ROOT_DIR, 'videos')\n",
    "    fidscs_globals.STICHED_VIDEO_FRAMES_DIR = os.path.join(fidscs_globals.DATA_ROOT_DIR, 'stitched_video_frames')\n",
    "    fidscs_globals.CORPUS_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.CORPUS_DS_FNAME)\n",
    "    fidscs_globals.DOCUMENT_ASL_CONSULTANT_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.DOCUMENT_ASL_CONSULTANT_DS_FNAME)\n",
    "    fidscs_globals.ASL_CONSULTANT_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.ASL_CONSULTANT_DS_FNAME)\n",
    "    fidscs_globals.VIDEO_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.VIDEO_DS_FNAME)\n",
    "    fidscs_globals.VIDEO_SEGMENT_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.VIDEO_SEGMENT_DS_FNAME)\n",
    "    fidscs_globals.VIDEO_FRAME_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.VIDEO_FRAME_DS_FNAME)\n",
    "    fidscs_globals.UTTERANCE_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.UTTERANCE_DS_FNAME)\n",
    "    fidscs_globals.UTTERANCE_VIDEO_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.UTTERANCE_VIDEO_DS_FNAME)\n",
    "    fidscs_globals.UTTERANCE_TOKEN_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.UTTERANCE_TOKEN_DS_FNAME)\n",
    "    fidscs_globals.UTTERANCE_TOKEN_FRAME_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.UTTERANCE_TOKEN_FRAME_DS_FNAME)\n",
    "    fidscs_globals.VOCABULARY_DS_PATH = os.path.join(fidscs_globals.DATA_ROOT_DIR, fidscs_globals.VOCABULARY_DS_FNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "pl = beam.Pipeline(options=pipeline_options)\n",
    "\n",
    "full_target_vid_index_schemad_pcoll = beam__common.pl__1__read_target_vid_index_csv(pl)\n",
    "corpus_index_schemad_pcoll = beam__common.pl__1__read_corpus_index_csv(pl) # XML is base-64 encode but we no longer need it (to decode it) since it is only used to create the datasets\n",
    "# corpus_index_decoded_XML_pcoll = pl__2__decode_XML(corpus_index_schemad_pcoll) # see above\n",
    "\n",
    "asl_consultant_index_schemad_pcoll = beam__common.pl__1__read_asl_consultant_index_csv(pl)\n",
    "document_asl_consultant_utterance_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_utterance_index_csv(pl)\n",
    "document_asl_consultant_target_video_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_target_video_index_csv(pl)\n",
    "document_asl_consultant_utterance_video_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_utterance_video_index_csv(pl)\n",
    "document_target_video_segment_index_schemad_pcoll = beam__common.pl__1__read_document_target_video_segment_index_csv(pl)\n",
    "vocabulary_index_schemad_pcoll = beam__common.pl__1__read_vocabulary_index_csv(pl)\n",
    "document_asl_consultant_utterance_token_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_utterance_token_index_csv(pl)\n",
    "document_asl_consultant_target_video_frame_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_target_video_frame_index_csv(pl)\n",
    "document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll = beam__common.pl__1__read_document_asl_consultant_target_video_utterance_token_frame_index_csv(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll is the main table we use for training.\n",
    "#     This will ultimately provide which frame sequences correspond to individual tokens.\n",
    "\n",
    "# But our first measure is to build train and validation sets (for tokens).\n",
    "#   In order to split up train vs validation sets, we need to compare \"apples to apples\".\n",
    "#   That is, in order for a token (TokenID) to be considered a candidate for the split,\n",
    "#   we require at least two of the same (TokenID, CameraPerspective) wherein the ASL\n",
    "#   consultant for each differs.  We would prefer more than two of these tuples, each\n",
    "#   having unique ASL consultants in the set of occurrences, with the majority of said\n",
    "#   tuples being assigned to the training set and the remainder (at least one) being\n",
    "#   assigned to the validation set.  We would like to achieve a 90/10 split, ideally,\n",
    "#   but we will take what we get.\n",
    "\n",
    "# document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll:\n",
    "    # beam.Row(\n",
    "    #   DocumentID=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[0]]),\n",
    "    #   ASLConsultantID=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[1]]),\n",
    "    #   CameraPerspective=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[2]]),\n",
    "    #   TargetVideoFilename=str(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[3]]),\n",
    "    #   UtteranceSequence=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[4]]),\n",
    "    #   TokenSequence=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[5]]),\n",
    "    #   FrameSequence=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[6]]),\n",
    "    #   TokenID=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[7]])\n",
    "    # )\n",
    "distinct_consultant_targetvideo_by_token_camera_perspective_pcoll = (\n",
    "    document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll\n",
    "    | \"Beam PL: extract ((TokenID, CameraPerspective), (ASLConsultantID, TargetVideoFilename)) from document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll\" >> beam.Map(\n",
    "            lambda schemad_pcoll_row: ((schemad_pcoll_row.TokenID, schemad_pcoll_row.CameraPerspective), (schemad_pcoll_row.ASLConsultantID, schemad_pcoll_row.TargetVideoFilename))\n",
    "        )\n",
    "    | \"Beam PL: select distinct ((TokenID, CameraPerspective), (ASLConsultantID, TargetVideoFilename)) from document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll\" >> beam.Distinct()\n",
    ")\n",
    "\n",
    "doc_consultant_targetvideo_utterance_tokenseq_frameseq_by_token_cameraperspective = (\n",
    "    document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll\n",
    "        # beam.Row(\n",
    "        #   DocumentID=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[0]]),\n",
    "        #   ASLConsultantID=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[1]]),\n",
    "        #   CameraPerspective=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[2]]),\n",
    "        #   TargetVideoFilename=str(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[3]]),\n",
    "        #   UtteranceSequence=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[4]]),\n",
    "        #   TokenSequence=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[5]]),\n",
    "        #   FrameSequence=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[6]]),\n",
    "        #   TokenID=int(d_document_asl_consultant_target_video_utterance_token_frame_info[fidscs_globals.SCHEMA_COL_NAMES__UTTERANCE_TOKEN_FRAME_DS[7]])\n",
    "        # )\n",
    "    | \"Beam PL: transform document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll to ((TokenID, CameraPerspective), (DocumentID, ASLConsultantID, TargetVideoFilename, UtteranceSequence, TokenSequence, FrameSequence))\" >> beam.Map(\n",
    "            lambda document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll_row: (\n",
    "                (\n",
    "                    document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll_row.TokenID,\n",
    "                    document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll_row.CameraPerspective\n",
    "                ),\n",
    "                (\n",
    "                    document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll_row.DocumentID,\n",
    "                    document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll_row.ASLConsultantID,\n",
    "                    document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll_row.TargetVideoFilename,\n",
    "                    document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll_row.UtteranceSequence,\n",
    "                    document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll_row.TokenSequence,\n",
    "                    document_asl_consultant_target_video_utterance_token_frame_index_schemad_pcoll_row.FrameSequence\n",
    "                )\n",
    "            )\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "consultant_target_videos_GROUPED_by_token_camera_perspective_pcoll = (\n",
    "    distinct_consultant_targetvideo_by_token_camera_perspective_pcoll\n",
    "    | \"Beam PL: group (ASLConsultantID, TargetVideoFilename) by (TokenID, CameraPerspective)\" >> beam.GroupByKey()\n",
    "    # the above produces tuples of the form:\n",
    "    #   ((<TokenID>, <CameraPerspective>), listof((<ASLConsultantID>, <TargetVideoFilename>)))\n",
    ")\n",
    "\n",
    "def flatten_ctvgbtcpp_tpl(ctvgbtcpp_tpl):\n",
    "    return [\n",
    "        (\n",
    "            ctvgbtcpp_tpl[0][0],            # TokenID\n",
    "            ctvgbtcpp_tpl[0][1],            # CameraPerspective\n",
    "            consultant_targetvideo_tpl[0],  # ASLConsultantID\n",
    "            consultant_targetvideo_tpl[1]   # TargetVideoFilename\n",
    "        ) for consultant_targetvideo_tpl in ctvgbtcpp_tpl[1]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_ctvgbtcpp__lte_1 = (\n",
    "    consultant_target_videos_GROUPED_by_token_camera_perspective_pcoll\n",
    "    | \"Beam PL: filter non-candidates for test-validation split\" >> beam.Filter(\n",
    "            lambda ctvgbtcppt: len(set(ctvgbtcppt[1]))<=1\n",
    "        )\n",
    "    | \"Beam PL: flatten filter non-candidates for test-validation split\" >> beam.FlatMap(flatten_ctvgbtcpp_tpl)\n",
    "    # the above produces tuples of the form:\n",
    "        # (\n",
    "        #     <TokenID>,                # TokenID\n",
    "        #     <CameraPerspective>,      # CameraPerspective\n",
    "        #     <ASLConsultantID>,        # ASLConsultantID\n",
    "        #     <TargetVideoFilename>     # TargetVideoFilename\n",
    "        # )\n",
    ")\n",
    "\n",
    "token_camera_perspective_keys__for__consultant_targetvideo__lte_1__pcoll = (\n",
    "    flattened_ctvgbtcpp__lte_1\n",
    "    # the above has tuples of the form:\n",
    "        # (\n",
    "        #     <TokenID>,                # TokenID\n",
    "        #     <CameraPerspective>,      # CameraPerspective\n",
    "        #     <ASLConsultantID>,        # ASLConsultantID\n",
    "        #     <TargetVideoFilename>     # TargetVideoFilename\n",
    "        # )\n",
    "    | \"Beam PL: extract ((TokenID, CameraPerspective), 'TokenID_CameraPerspective___with__ASLConsultantID_TargetVideoFilename__lte_1') from flattened_ctvgbtcpp__lte_1\" >> beam.Map(\n",
    "            lambda flattened_ctvgbtcpp__lte_1_tpl: ((flattened_ctvgbtcpp__lte_1_tpl[0], flattened_ctvgbtcpp__lte_1_tpl[1]), 'TokenID_CameraPerspective___with__ASLConsultantID_TargetVideoFilename__lte_1')\n",
    "        )\n",
    "    | \"Beam PL: select distinct ((TokenID, CameraPerspective), 'TokenID_CameraPerspective___with__ASLConsultantID_TargetVideoFilename__lte_1') from flattened_ctvgbtcpp__lte_1\" >> beam.Distinct()\n",
    ")\n",
    "\n",
    "doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__lte_1 = (\n",
    "    ({\n",
    "      'token_camera_perspective_keys__for__consultant_targetvideo__lte_1__pcoll': token_camera_perspective_keys__for__consultant_targetvideo__lte_1__pcoll,\n",
    "      'doc_consultant_targetvideo_utterance_tokenseq_frameseq_map': doc_consultant_targetvideo_utterance_tokenseq_frameseq_by_token_cameraperspective\n",
    "    })\n",
    "    | \"Beam PL: join token_camera_perspective_keys__for__consultant_targetvideo__lte_1__pcoll with doc_consultant_targetvideo_utterance_tokenseq_frameseq_by_token_cameraperspective\" >> beam.CoGroupByKey()\n",
    "        # the above produces tuples of the form:\n",
    "        # (\n",
    "        #     (<TokenID>, <CameraPerspective>), # key\n",
    "        #     {\n",
    "        #         'token_camera_perspective_keys__for__consultant_targetvideo__lte_1__pcoll': listof('TokenID_CameraPerspective___with__ASLConsultantID_TargetVideoFilename__lte_1'),\n",
    "        #         'doc_consultant_targetvideo_utterance_tokenseq_frameseq_map': listof(\n",
    "        #             (\n",
    "        #                 <DocumentID>,\n",
    "        #                 <ASLConsultantID>,\n",
    "        #                 <TargetVideoFilename>,\n",
    "        #                 <UtteranceSequence>,\n",
    "        #                 <TokenSequence>,\n",
    "        #                 <FrameSequence>\n",
    "        #             )\n",
    "        #         )\n",
    "        #     }\n",
    "        # )\n",
    "    | \"Beam PL: 'explode' doc_consultant_targetvideo_utterance_tokenseq_frameseq_map in tcptdctvustsfsm__lte_1__tpl to list of tuples\" >> beam.Map(\n",
    "            lambda tcptdctvustsfsm__lte_1__tpl: [\n",
    "                (\n",
    "                    doc_consultant_targetvideo_utterance_tokenseq_frameseq_tpl[0],  # <DocumentID>\n",
    "                    doc_consultant_targetvideo_utterance_tokenseq_frameseq_tpl[1],  # <ASLConsultantID>\n",
    "                    doc_consultant_targetvideo_utterance_tokenseq_frameseq_tpl[2],  # <TargetVideoFilename>\n",
    "                    doc_consultant_targetvideo_utterance_tokenseq_frameseq_tpl[3],  # <UtteranceSequence>\n",
    "                    doc_consultant_targetvideo_utterance_tokenseq_frameseq_tpl[4],  # <TokenSequence>\n",
    "                    doc_consultant_targetvideo_utterance_tokenseq_frameseq_tpl[5],  # <FrameSequence>\n",
    "                    tcptdctvustsfsm__lte_1__tpl[0][1],                              # <CameraPerspective>\n",
    "                    tcptdctvustsfsm__lte_1__tpl[0][0]                               # <TokenID>\n",
    "                ) for doc_consultant_targetvideo_utterance_tokenseq_frameseq_tpl in tcptdctvustsfsm__lte_1__tpl[1]['doc_consultant_targetvideo_utterance_tokenseq_frameseq_map']\n",
    "            ]\n",
    "        )\n",
    "    | \"Beam PL: 'explode' list_tcptdctvustsfsm__lte_1__tpl to tuples\" >> beam.FlatMap(lambda list_dctvustscptifs__lte_1__tpl: list_dctvustscptifs__lte_1__tpl)\n",
    "  )\n",
    "doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__lte_1 = beam__common.pl__X__sort_pcoll(doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__lte_1, \"doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__lte_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_ctvgbtcpp__gt_1 = (\n",
    "    consultant_target_videos_GROUPED_by_token_camera_perspective_pcoll\n",
    "    | \"Beam PL: filter candidates for test-validation split\" >> beam.Filter(\n",
    "            lambda ctvgbtcppt: len(set(ctvgbtcppt[1]))>1\n",
    "        )\n",
    "    | \"Beam PL: flatten filter candidates for test-validation split\" >> beam.FlatMap(flatten_ctvgbtcpp_tpl)\n",
    "    # the above produces tuples of the form:\n",
    "        # (\n",
    "        #     <TokenID>,                # TokenID\n",
    "        #     <CameraPerspective>,      # CameraPerspective\n",
    "        #     <ASLConsultantID>,        # ASLConsultantID\n",
    "        #     <TargetVideoFilename>     # TargetVideoFilename\n",
    "        # )\n",
    ")\n",
    "\n",
    "token_camera_perspective_keys__for__consultant_targetvideo__gt_1__pcoll = (\n",
    "    flattened_ctvgbtcpp__gt_1\n",
    "    # the above has tuples of the form:\n",
    "        # (\n",
    "        #     <TokenID>,                # TokenID\n",
    "        #     <CameraPerspective>,      # CameraPerspective\n",
    "        #     <ASLConsultantID>,        # ASLConsultantID\n",
    "        #     <TargetVideoFilename>     # TargetVideoFilename\n",
    "        # )\n",
    "    | \"Beam PL: extract ((TokenID, CameraPerspective), 'TokenID_CameraPerspective___with__ASLConsultantID_TargetVideoFilename__gt_1') from flattened_ctvgbtcpp__gt_1\" >> beam.Map(\n",
    "            lambda flattened_ctvgbtcpp__gt_1_tpl: ((flattened_ctvgbtcpp__gt_1_tpl[0], flattened_ctvgbtcpp__gt_1_tpl[1]), 'TokenID_CameraPerspective___with__ASLConsultantID_TargetVideoFilename__gt_1')\n",
    "        )\n",
    "    | \"Beam PL: select distinct ((TokenID, CameraPerspective), 'TokenID_CameraPerspective___with__ASLConsultantID_TargetVideoFilename__gt_1') from flattened_ctvgbtcpp__gt_1\" >> beam.Distinct()\n",
    ")\n",
    "\n",
    "doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__gt_1 = (\n",
    "    ({\n",
    "      'token_camera_perspective_keys__for__consultant_targetvideo__gt_1__pcoll': token_camera_perspective_keys__for__consultant_targetvideo__gt_1__pcoll,\n",
    "      'doc_consultant_targetvideo_utterance_tokenseq_frameseq_map': doc_consultant_targetvideo_utterance_tokenseq_frameseq_by_token_cameraperspective\n",
    "    })\n",
    "    | \"Beam PL: join token_camera_perspective_keys__for__consultant_targetvideo__gt_1__pcoll with doc_consultant_targetvideo_utterance_tokenseq_frameseq_by_token_cameraperspective\" >> beam.CoGroupByKey()\n",
    "        # the above produces tuples of the form:\n",
    "        # (\n",
    "        #     (<TokenID>, <CameraPerspective>), # key\n",
    "        #     {\n",
    "        #         'token_camera_perspective_keys__for__consultant_targetvideo__gt_1__pcoll': listof('TokenID_CameraPerspective___with__ASLConsultantID_TargetVideoFilename__gt_1'),\n",
    "        #         'doc_consultant_targetvideo_utterance_tokenseq_frameseq_map': listof(\n",
    "        #             (\n",
    "        #                 <DocumentID>,\n",
    "        #                 <ASLConsultantID>,\n",
    "        #                 <TargetVideoFilename>,\n",
    "        #                 <UtteranceSequence>,\n",
    "        #                 <TokenSequence>,\n",
    "        #                 <FrameSequence>\n",
    "        #             )\n",
    "        #         )\n",
    "        #     }\n",
    "        # )\n",
    "    | \"Beam PL: 'explode' doc_consultant_targetvideo_utterance_tokenseq_frameseq_map in tcptdctvustsfsm__gt_1__tpl to list of tuples\" >> beam.Map(\n",
    "            lambda tcptdctvustsfsm__gt_1__tpl: [\n",
    "                (\n",
    "                    doc_consultant_targetvideo_utterance_tokenseq_frameseq_tpl[0],  # <DocumentID>\n",
    "                    doc_consultant_targetvideo_utterance_tokenseq_frameseq_tpl[1],  # <ASLConsultantID>\n",
    "                    doc_consultant_targetvideo_utterance_tokenseq_frameseq_tpl[2],  # <TargetVideoFilename>\n",
    "                    doc_consultant_targetvideo_utterance_tokenseq_frameseq_tpl[3],  # <UtteranceSequence>\n",
    "                    doc_consultant_targetvideo_utterance_tokenseq_frameseq_tpl[4],  # <TokenSequence>\n",
    "                    doc_consultant_targetvideo_utterance_tokenseq_frameseq_tpl[5],  # <FrameSequence>\n",
    "                    tcptdctvustsfsm__gt_1__tpl[0][1],                               # <CameraPerspective>\n",
    "                    tcptdctvustsfsm__gt_1__tpl[0][0]                                # <TokenID>\n",
    "                ) for doc_consultant_targetvideo_utterance_tokenseq_frameseq_tpl in tcptdctvustsfsm__gt_1__tpl[1]['doc_consultant_targetvideo_utterance_tokenseq_frameseq_map']\n",
    "            ]\n",
    "        )\n",
    "    | \"Beam PL: 'explode' list_tcptdctvustsfsm__gt_1__tpl to tuples\" >> beam.FlatMap(lambda list_dctvustscptifs__gt_1__tpl: list_dctvustscptifs__gt_1__tpl)\n",
    "  )\n",
    "doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__gt_1 = beam__common.pl__X__sort_pcoll(doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__gt_1, \"doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__gt_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_asl_consultant_utterance_token_index_schemad_pcoll\n",
    "    # beam.Row(\n",
    "    #     DocumentID=document_asl_consultant_utterance_token_tpl[0],\n",
    "    #     DocumentFilename=document_asl_consultant_utterance_token_tpl[1],\n",
    "    #     ASLConsultantID=document_asl_consultant_utterance_token_tpl[2],\n",
    "    #     ParticipantName=document_asl_consultant_utterance_token_tpl[3],\n",
    "    #     UtteranceSequence=document_asl_consultant_utterance_token_tpl[4],\n",
    "    #     TokenSequence=document_asl_consultant_utterance_token_tpl[7],\n",
    "    #     StartTime=document_asl_consultant_utterance_token_tpl[8],\n",
    "    #     EndTime=document_asl_consultant_utterance_token_tpl[9],\n",
    "    #     TokenID=document_asl_consultant_utterance_token_tpl[5],\n",
    "    #     Field='', # blank for now\n",
    "    #     FieldValue='' # blank for now\n",
    "    # )\n",
    "\n",
    "# join:\n",
    "    # document_asl_consultant_utterance_token_index_schemad_pcoll:  \n",
    "        # beam.Row(\n",
    "        #     <DocumentID>,\n",
    "        #     <DocumentFilename>,\n",
    "        #     <ASLConsultantID>,\n",
    "        #     <ParticipantName>,\n",
    "        #     <UtteranceSequence>,\n",
    "        #     <TokenSequence>,\n",
    "        #     <StartTime>,\n",
    "        #     <EndTime>,\n",
    "        #     <TokenID>,\n",
    "        #     <Field>,\n",
    "        #     <FieldValue>\n",
    "        # )\n",
    "\n",
    "        # keyed by:\n",
    "            # (<DocumentID>, <ASLConsultantID>, <UtteranceSequence>, <TokenSequence>, <TokenID>)\n",
    "\n",
    "        # we'll call this data set:\n",
    "            # document_consultant_completeutterance_tokenseq_token_keys\n",
    "document_consultant_completeutterance_tokenseq_token_keys = (\n",
    "    document_asl_consultant_utterance_token_index_schemad_pcoll\n",
    "    | \"Beam PL: extract (<DocumentID>, <ASLConsultantID>, <UtteranceSequence>, <TokenSequence>, <TokenID>) from document_asl_consultant_utterance_token_index_schemad_pcoll\" >> beam.Map(\n",
    "            lambda document_asl_consultant_utterance_token_index_schemad_pcoll_row: (\n",
    "                document_asl_consultant_utterance_token_index_schemad_pcoll_row.DocumentID,\n",
    "                document_asl_consultant_utterance_token_index_schemad_pcoll_row.ASLConsultantID,\n",
    "                document_asl_consultant_utterance_token_index_schemad_pcoll_row.UtteranceSequence,\n",
    "                document_asl_consultant_utterance_token_index_schemad_pcoll_row.TokenSequence,\n",
    "                document_asl_consultant_utterance_token_index_schemad_pcoll_row.TokenID\n",
    "            )\n",
    "        )\n",
    ")\n",
    "document_consultant_completeutterance_tokenseq_token_keys = beam__common.pl__X__sort_pcoll(document_consultant_completeutterance_tokenseq_token_keys, \"document_consultant_completeutterance_tokenseq_token_keys\")\n",
    "\n",
    "    # to \n",
    "\n",
    "    # doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__gt_1:\n",
    "        # (\n",
    "        #     <DocumentID>,\n",
    "        #     <ASLConsultantID>,\n",
    "        #     <TargetVideoFilename>,\n",
    "        #     <UtteranceSequence>,\n",
    "        #     <TokenSequence>,\n",
    "        #     <FrameSequence>,\n",
    "        #     <CameraPerspective>,\n",
    "        #     <TokenID>,\n",
    "        # )\n",
    "\n",
    "        # keyed by:\n",
    "            # (<DocumentID>, <ASLConsultantID>, <UtteranceSequence>, <TokenSequence>, <TokenID>)\n",
    "\n",
    "        # data:\n",
    "            # (<FrameSequence>, <TargetVideoFilename>, <CameraPerspective>)\n",
    "\n",
    "        # we'll call this data set:\n",
    "            # frameseq_targetvideo_cameraperspective__by__document_consultant_utterance_tokenseq_token__gt_1\n",
    "frameseq_targetvideo_cameraperspective__by__document_consultant_utterance_tokenseq_token__gt_1 = (\n",
    "    doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__gt_1\n",
    "    | \"Beam PL: extract ((<DocumentID>, <ASLConsultantID>, <UtteranceSequence>, <TokenSequence>, <TokenID>), (<FrameSequence>, <TargetVideoFilename>, <CameraPerspective>)) from doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__gt_1\" >> beam.Map(\n",
    "            lambda doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__gt_1_tpl: (\n",
    "                (\n",
    "                    doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__gt_1_tpl[0],   # <DocumentID>\n",
    "                    doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__gt_1_tpl[1],   # <ASLConsultantID>\n",
    "                    doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__gt_1_tpl[3],   # <UtteranceSequence>\n",
    "                    doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__gt_1_tpl[4],   # <TokenSequence>\n",
    "                    doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__gt_1_tpl[7]    # <TokenID>\n",
    "                ),\n",
    "                (\n",
    "                    doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__gt_1_tpl[5],   # <FrameSequence>\n",
    "                    doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__gt_1_tpl[2],   # <TargetVideoFilename>\n",
    "                    doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__gt_1_tpl[6],   # <CameraPerspective>\n",
    "                )\n",
    "            )\n",
    "        )\n",
    ")\n",
    "\n",
    "# we MUST find all records from frameseq_targetvideo_cameraperspective__by__document_consultant_utterance_tokenseq_token__gt_1\n",
    "    # with COMPLETE sequences in document_consultant_completeutterance_tokenseq_token_keys\n",
    "\n",
    "    # we'll call this data set:\n",
    "        # doc_consultant_utteranceseq_tokenseq_frameseq_targetvideo_cameraperspective_token__gt_1\n",
    "doc_consultant_utteranceseq_tokenseq_frameseq_targetvideo_cameraperspective_token__gt_1 = (\n",
    "    frameseq_targetvideo_cameraperspective__by__document_consultant_utterance_tokenseq_token__gt_1\n",
    "    | \"Beam PL: filter tuples from fstvcpbdcustst_gt_1 with complete utterances\" >> beam.Filter(\n",
    "        lambda fstvcpbdcustst_gt_1_entry, matching_document_consultant_completeutterance_tokenseq_token_keys: (fstvcpbdcustst_gt_1_entry[0][0], fstvcpbdcustst_gt_1_entry[0][1], fstvcpbdcustst_gt_1_entry[0][2], fstvcpbdcustst_gt_1_entry[0][3], fstvcpbdcustst_gt_1_entry[0][4]) in matching_document_consultant_completeutterance_tokenseq_token_keys,\n",
    "        matching_document_consultant_completeutterance_tokenseq_token_keys=beam.pvalue.AsIter(document_consultant_completeutterance_tokenseq_token_keys),\n",
    "      )\n",
    "    | \"Beam PL: transform filtered tuples from fstvcpbdcustst_gt_1 with complete utterances to (<DocumentID>, <ASLConsultantID>, <UtteranceSequence>, <TokenSequence>, <FrameSequence>, <TargetVideoFilename>, <CameraPerspective>, <TokenID>)\" >> beam.Map(\n",
    "            lambda fstvcpbdcustst_gt_1_entry: (\n",
    "                fstvcpbdcustst_gt_1_entry[0][0],    # <DocumentID>\n",
    "                fstvcpbdcustst_gt_1_entry[0][1],    # <ASLConsultantID>\n",
    "                fstvcpbdcustst_gt_1_entry[0][2],    # <UtteranceSequence>\n",
    "                fstvcpbdcustst_gt_1_entry[0][3],    # <TokenSequence>\n",
    "                fstvcpbdcustst_gt_1_entry[1][0],    # <FrameSequence>\n",
    "                fstvcpbdcustst_gt_1_entry[1][1],    # <TargetVideoFilename>\n",
    "                fstvcpbdcustst_gt_1_entry[1][2],    # <CameraPerspective>\n",
    "                fstvcpbdcustst_gt_1_entry[0][4]     # <TokenID>\n",
    "            )\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we require this in order to make use of ib.show() (which provides visualization/to_df of the pcolls specified) or ib.collect() (which creates a pandas dataframe from a pcoll)\n",
    "    # but all pcolls we wish to visualize must be created prior to executing the following line\n",
    "ib.watch(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n            <div id=\"progress_indicator_83ad560e1f329123f073bbf88e6de895\" class=\"spinner-border text-info\" role=\"status\">\n            </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n            $(\"#progress_indicator_83ad560e1f329123f073bbf88e6de895\").remove();\n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n            $(\"#progress_indicator_83ad560e1f329123f073bbf88e6de895\").remove();\n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# ib.show(ctvgbtcpp__gt_1, visualize_data=True)\n",
    "df_ctvgbtcpp__gt_1 = ib.collect(flattened_ctvgbtcpp__gt_1)\n",
    "# ib.evict_recorded_data(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           ASLConsultantID            TargetVideoFilename\n",
       "TokenID CameraPerspective                                                \n",
       "0       0                                4     roadtrip2_1051_small_0.mov\n",
       "        0                                1      ben_story_439_small_0.mov\n",
       "        0                                1      ben_story_441_small_0.mov\n",
       "        1                                1      ben_story_439_small_1.mov\n",
       "        1                                1      ben_story_441_small_1.mov\n",
       "...                                    ...                            ...\n",
       "2375    1                                4              _1450_small_1.mov\n",
       "        3                                4              _1397_small_3.mov\n",
       "        3                                4              _1450_small_3.mov\n",
       "2401    0                                4  muhammed_ali_1052_small_0.mov\n",
       "        0                                5         DSP%2520Immigrants.mov\n",
       "\n",
       "[18501 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th>0</th>\n      <td>4</td>\n      <td>roadtrip2_1051_small_0.mov</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_439_small_0.mov</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_441_small_0.mov</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>ben_story_439_small_1.mov</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>ben_story_441_small_1.mov</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">2375</th>\n      <th>1</th>\n      <td>4</td>\n      <td>_1450_small_1.mov</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>_1397_small_3.mov</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>_1450_small_3.mov</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">2401</th>\n      <th>0</th>\n      <td>4</td>\n      <td>muhammed_ali_1052_small_0.mov</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>DSP%2520Immigrants.mov</td>\n    </tr>\n  </tbody>\n</table>\n<p>18501 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "df_ctvgbtcpp__gt_1.columns = ['TokenID', 'CameraPerspective', 'ASLConsultantID', 'TargetVideoFilename']\n",
    "df_ctvgbtcpp__gt_1.set_index(['TokenID', 'CameraPerspective'], inplace=True)\n",
    "df_ctvgbtcpp__gt_1.sort_index(inplace=True)\n",
    "df_ctvgbtcpp__gt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           count\n",
       "TokenID CameraPerspective       \n",
       "0       0                      3\n",
       "        1                      2\n",
       "        2                      3\n",
       "        3                      3\n",
       "1       0                      5\n",
       "...                          ...\n",
       "2357    2                      2\n",
       "2375    0                      2\n",
       "        1                      2\n",
       "        3                      2\n",
       "2401    0                      2\n",
       "\n",
       "[2177 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>count</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">0</th>\n      <th>0</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>0</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2357</th>\n      <th>2</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">2375</th>\n      <th>0</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2401</th>\n      <th>0</th>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>2177 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df_ctvgbtcpp__gt_1__count = df_ctvgbtcpp__gt_1.reset_index().groupby(['TokenID', 'CameraPerspective']).count()\n",
    "df_ctvgbtcpp__gt_1__count = df_ctvgbtcpp__gt_1__count[['ASLConsultantID']]\n",
    "df_ctvgbtcpp__gt_1__count.columns = ['count']\n",
    "df_ctvgbtcpp__gt_1__count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n            <div id=\"progress_indicator_645fa7a1ac4036d94b0ebc45a717793f\" class=\"spinner-border text-info\" role=\"status\">\n            </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n            $(\"#progress_indicator_645fa7a1ac4036d94b0ebc45a717793f\").remove();\n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n            $(\"#progress_indicator_645fa7a1ac4036d94b0ebc45a717793f\").remove();\n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_dctvustscptifs__gt_1 = ib.collect(doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__gt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                                                                         TokenID\n",
       "DocumentID ASLConsultantID TargetVideoFilename          CameraPerspective UtteranceSequence TokenSequence FrameSequence         \n",
       "0          1               ben_story_439_small_0.mov    0                 0                 0             20                 935\n",
       "                                                                                                          21                 935\n",
       "                                                                                            1             31                 728\n",
       "                                                                                                          32                 728\n",
       "                                                                                                          33                 728\n",
       "...                                                                                                                          ...\n",
       "37         4               biker_buddy_1069_small_2.mov 2                 15                14            2359               676\n",
       "                                                                                                          2360               676\n",
       "                                                                                                          2361               676\n",
       "                                                                                                          2362               676\n",
       "                                                                                                          2363               676\n",
       "\n",
       "[290626 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>TokenID</th>\n    </tr>\n    <tr>\n      <th>DocumentID</th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>CameraPerspective</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n      <th>FrameSequence</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th rowspan=\"5\" valign=\"top\">1</th>\n      <th rowspan=\"5\" valign=\"top\">ben_story_439_small_0.mov</th>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th rowspan=\"2\" valign=\"top\">0</th>\n      <th>20</th>\n      <td>935</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>935</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">1</th>\n      <th>31</th>\n      <td>728</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>728</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>728</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">37</th>\n      <th rowspan=\"5\" valign=\"top\">4</th>\n      <th rowspan=\"5\" valign=\"top\">biker_buddy_1069_small_2.mov</th>\n      <th rowspan=\"5\" valign=\"top\">2</th>\n      <th rowspan=\"5\" valign=\"top\">15</th>\n      <th rowspan=\"5\" valign=\"top\">14</th>\n      <th>2359</th>\n      <td>676</td>\n    </tr>\n    <tr>\n      <th>2360</th>\n      <td>676</td>\n    </tr>\n    <tr>\n      <th>2361</th>\n      <td>676</td>\n    </tr>\n    <tr>\n      <th>2362</th>\n      <td>676</td>\n    </tr>\n    <tr>\n      <th>2363</th>\n      <td>676</td>\n    </tr>\n  </tbody>\n</table>\n<p>290626 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "df_dctvustscptifs__gt_1.columns = ['DocumentID', 'ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence', 'FrameSequence', 'CameraPerspective', 'TokenID']\n",
    "df_dctvustscptifs__gt_1.set_index(['DocumentID', 'ASLConsultantID', 'TargetVideoFilename', 'CameraPerspective', 'UtteranceSequence', 'TokenSequence', 'FrameSequence'], inplace=True)\n",
    "df_dctvustscptifs__gt_1.sort_index(inplace=True)\n",
    "df_dctvustscptifs__gt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n            <div id=\"progress_indicator_098a7fc11e69e9724e59047c4d28f82c\" class=\"spinner-border text-info\" role=\"status\">\n            </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n            $(\"#progress_indicator_098a7fc11e69e9724e59047c4d28f82c\").remove();\n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n            $(\"#progress_indicator_098a7fc11e69e9724e59047c4d28f82c\").remove();\n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_ctvgbtcpp__lte_1 = ib.collect(flattened_ctvgbtcpp__lte_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           ASLConsultantID           TargetVideoFilename\n",
       "TokenID CameraPerspective                                               \n",
       "2       0                                1     ben_story_439_small_0.mov\n",
       "        1                                1     ben_story_439_small_1.mov\n",
       "        2                                1     ben_story_439_small_2.mov\n",
       "        3                                1     ben_story_439_small_3.mov\n",
       "12      0                                4   dorm_prank_1053_small_0.mov\n",
       "...                                    ...                           ...\n",
       "2409    0                                5        DSP%2520Immigrants.mov\n",
       "2410    0                                4    boston-la_1088_small_0.mov\n",
       "        2                                4    boston-la_1088_small_2.mov\n",
       "2411    0                                4  biker_buddy_1069_small_0.mov\n",
       "        2                                4  biker_buddy_1069_small_2.mov\n",
       "\n",
       "[4611 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">2</th>\n      <th>0</th>\n      <td>1</td>\n      <td>ben_story_439_small_0.mov</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>ben_story_439_small_1.mov</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>ben_story_439_small_2.mov</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>ben_story_439_small_3.mov</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <th>0</th>\n      <td>4</td>\n      <td>dorm_prank_1053_small_0.mov</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2409</th>\n      <th>0</th>\n      <td>5</td>\n      <td>DSP%2520Immigrants.mov</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">2410</th>\n      <th>0</th>\n      <td>4</td>\n      <td>boston-la_1088_small_0.mov</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>boston-la_1088_small_2.mov</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">2411</th>\n      <th>0</th>\n      <td>4</td>\n      <td>biker_buddy_1069_small_0.mov</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>biker_buddy_1069_small_2.mov</td>\n    </tr>\n  </tbody>\n</table>\n<p>4611 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "df_ctvgbtcpp__lte_1.columns = ['TokenID', 'CameraPerspective', 'ASLConsultantID', 'TargetVideoFilename']\n",
    "df_ctvgbtcpp__lte_1.set_index(['TokenID', 'CameraPerspective'], inplace=True)\n",
    "df_ctvgbtcpp__lte_1.sort_index(inplace=True)\n",
    "df_ctvgbtcpp__lte_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           count\n",
       "TokenID CameraPerspective       \n",
       "2       0                      1\n",
       "        1                      1\n",
       "        2                      1\n",
       "        3                      1\n",
       "12      0                      1\n",
       "...                          ...\n",
       "2409    0                      1\n",
       "2410    0                      1\n",
       "        2                      1\n",
       "2411    0                      1\n",
       "        2                      1\n",
       "\n",
       "[4611 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>count</th>\n    </tr>\n    <tr>\n      <th>TokenID</th>\n      <th>CameraPerspective</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">2</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2409</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">2410</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">2411</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>4611 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "df_ctvgbtcpp__lte_1_count = df_ctvgbtcpp__lte_1.reset_index().groupby(['TokenID', 'CameraPerspective']).count()\n",
    "df_ctvgbtcpp__lte_1_count = df_ctvgbtcpp__lte_1_count[['ASLConsultantID']]\n",
    "df_ctvgbtcpp__lte_1_count.columns = ['count']\n",
    "df_ctvgbtcpp__lte_1_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n            <div id=\"progress_indicator_8e30ff1a6061a444e250caaa4e58b465\" class=\"spinner-border text-info\" role=\"status\">\n            </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n            $(\"#progress_indicator_8e30ff1a6061a444e250caaa4e58b465\").remove();\n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n            $(\"#progress_indicator_8e30ff1a6061a444e250caaa4e58b465\").remove();\n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_dctvustscptifs__lte_1 = ib.collect(doc_consultant_targetvideo_utteranceseq_tokenseq_cameraperspective_tokenid_frameseq__lte_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                                                                         TokenID\n",
       "DocumentID ASLConsultantID TargetVideoFilename          CameraPerspective UtteranceSequence TokenSequence FrameSequence         \n",
       "0          1               ben_story_439_small_0.mov    0                 0                 0             20                 935\n",
       "                                                                                                          21                 935\n",
       "                                                                                            1             31                 728\n",
       "                                                                                                          32                 728\n",
       "                                                                                                          33                 728\n",
       "...                                                                                                                          ...\n",
       "37         4               biker_buddy_1069_small_2.mov 2                 15                14            2359               676\n",
       "                                                                                                          2360               676\n",
       "                                                                                                          2361               676\n",
       "                                                                                                          2362               676\n",
       "                                                                                                          2363               676\n",
       "\n",
       "[290043 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>TokenID</th>\n    </tr>\n    <tr>\n      <th>DocumentID</th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>CameraPerspective</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n      <th>FrameSequence</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th rowspan=\"5\" valign=\"top\">1</th>\n      <th rowspan=\"5\" valign=\"top\">ben_story_439_small_0.mov</th>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th rowspan=\"2\" valign=\"top\">0</th>\n      <th>20</th>\n      <td>935</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>935</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">1</th>\n      <th>31</th>\n      <td>728</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>728</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>728</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">37</th>\n      <th rowspan=\"5\" valign=\"top\">4</th>\n      <th rowspan=\"5\" valign=\"top\">biker_buddy_1069_small_2.mov</th>\n      <th rowspan=\"5\" valign=\"top\">2</th>\n      <th rowspan=\"5\" valign=\"top\">15</th>\n      <th rowspan=\"5\" valign=\"top\">14</th>\n      <th>2359</th>\n      <td>676</td>\n    </tr>\n    <tr>\n      <th>2360</th>\n      <td>676</td>\n    </tr>\n    <tr>\n      <th>2361</th>\n      <td>676</td>\n    </tr>\n    <tr>\n      <th>2362</th>\n      <td>676</td>\n    </tr>\n    <tr>\n      <th>2363</th>\n      <td>676</td>\n    </tr>\n  </tbody>\n</table>\n<p>290043 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "df_dctvustscptifs__lte_1.columns = ['DocumentID', 'ASLConsultantID', 'TargetVideoFilename', 'UtteranceSequence', 'TokenSequence', 'FrameSequence', 'CameraPerspective', 'TokenID']\n",
    "df_dctvustscptifs__lte_1.set_index(['DocumentID', 'ASLConsultantID', 'TargetVideoFilename', 'CameraPerspective', 'UtteranceSequence', 'TokenSequence', 'FrameSequence'], inplace=True)\n",
    "df_dctvustscptifs__lte_1.sort_index(inplace=True)\n",
    "df_dctvustscptifs__lte_1"
   ]
  },
  {
   "source": [
    "#### Now we need to find all tokens corresponding to complete utterances from the filtered tokens with more than one occurrence (of unique consultant/camera perspectives)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n            <div id=\"progress_indicator_955226ec7c29a6f7b704963b60102606\" class=\"spinner-border text-info\" role=\"status\">\n            </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n            $(\"#progress_indicator_955226ec7c29a6f7b704963b60102606\").remove();\n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n            $(\"#progress_indicator_955226ec7c29a6f7b704963b60102606\").remove();\n          });\n        }"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_dcustsfstvcpt__gt_1 = ib.collect(doc_consultant_utteranceseq_tokenseq_frameseq_targetvideo_cameraperspective_token__gt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                                                                         TokenID\n",
       "DocumentID ASLConsultantID TargetVideoFilename          CameraPerspective UtteranceSequence TokenSequence FrameSequence         \n",
       "0          1               ben_story_439_small_0.mov    0                 0                 0             20                 935\n",
       "                                                                                                          21                 935\n",
       "                                                                                            1             31                 728\n",
       "                                                                                                          32                 728\n",
       "                                                                                                          33                 728\n",
       "...                                                                                                                          ...\n",
       "37         4               biker_buddy_1069_small_2.mov 2                 15                14            2359               676\n",
       "                                                                                                          2360               676\n",
       "                                                                                                          2361               676\n",
       "                                                                                                          2362               676\n",
       "                                                                                                          2363               676\n",
       "\n",
       "[290626 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>TokenID</th>\n    </tr>\n    <tr>\n      <th>DocumentID</th>\n      <th>ASLConsultantID</th>\n      <th>TargetVideoFilename</th>\n      <th>CameraPerspective</th>\n      <th>UtteranceSequence</th>\n      <th>TokenSequence</th>\n      <th>FrameSequence</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th rowspan=\"5\" valign=\"top\">1</th>\n      <th rowspan=\"5\" valign=\"top\">ben_story_439_small_0.mov</th>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th rowspan=\"2\" valign=\"top\">0</th>\n      <th>20</th>\n      <td>935</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>935</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">1</th>\n      <th>31</th>\n      <td>728</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>728</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>728</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">37</th>\n      <th rowspan=\"5\" valign=\"top\">4</th>\n      <th rowspan=\"5\" valign=\"top\">biker_buddy_1069_small_2.mov</th>\n      <th rowspan=\"5\" valign=\"top\">2</th>\n      <th rowspan=\"5\" valign=\"top\">15</th>\n      <th rowspan=\"5\" valign=\"top\">14</th>\n      <th>2359</th>\n      <td>676</td>\n    </tr>\n    <tr>\n      <th>2360</th>\n      <td>676</td>\n    </tr>\n    <tr>\n      <th>2361</th>\n      <td>676</td>\n    </tr>\n    <tr>\n      <th>2362</th>\n      <td>676</td>\n    </tr>\n    <tr>\n      <th>2363</th>\n      <td>676</td>\n    </tr>\n  </tbody>\n</table>\n<p>290626 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "df_dcustsfstvcpt__gt_1.columns = ['DocumentID', 'ASLConsultantID', 'UtteranceSequence', 'TokenSequence', 'FrameSequence', 'TargetVideoFilename', 'CameraPerspective', 'TokenID']\n",
    "df_dcustsfstvcpt__gt_1.set_index(['DocumentID', 'ASLConsultantID', 'TargetVideoFilename', 'CameraPerspective', 'UtteranceSequence', 'TokenSequence', 'FrameSequence'], inplace=True)\n",
    "df_dcustsfstvcpt__gt_1.sort_index(inplace=True)\n",
    "df_dcustsfstvcpt__gt_1"
   ]
  }
 ]
}